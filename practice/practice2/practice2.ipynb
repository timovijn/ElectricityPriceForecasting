{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + Tensorflow and Hypteropt Python tutorial\n",
    "Made by Ties van der Heijden, TU Delft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will contintue with Dutch DAM price forecasting. This time we will give a detailed specification of our Neural Network, and we will optimize hyperparameters using HyperOpt.\n",
    "\n",
    "To do this, the following packages are necessary:\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Matplotlib\n",
    "- Tensorflow\n",
    "\n",
    "And some specific functions are handy:\n",
    "- SciKit Learn: KFold, StandardScaler\n",
    "- Pathlib: Path\n",
    "\n",
    "\n",
    "PS: Be sure to create a new environment for TF + Keras + Hyperopt, since pip and anaconda don't work too well together and can cause errors in the future. Better to have them conflict in a new python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\u001b[32mFrom this point onward, the exercise was performed by Timo Vijn\u001b[0m\n\n\u001b[32mv1.0 (2021, Jan 20)\u001b[0m\n\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "print(), print(colored('From this point onward, the exercise was performed by Timo Vijn', 'green')), print();\n",
    "\n",
    "print(colored('v1.0 (2021, Jan 20)', 'green')), print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\u001b[32mFinished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "from keras import backend\n",
    "\n",
    "print(''), print(colored('Finished','green'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model creation - MLP\n",
    "\n",
    "Start the function with the following command:\n",
    "```python\n",
    "tensorflow.keras.backend.clear_session()\n",
    "```\n",
    "Else Keras will keep all the trained models stored in the RAM, which causes the memory to slowly fill up.<br>\n",
    "\n",
    "First we will build a function that returns a keras MLP as a function of its hyperparameters. Use the following hyperparameters:\n",
    "- Hidden nodes in layer 1\n",
    "- Hidden nodes in layer 2\n",
    "- Activation function of the hidden layers\n",
    "- Loss function (see keras.losses)\n",
    "- Dropout rate\n",
    "- Weight initialization (see keras.initializers)\n",
    "\n",
    "We will fix some things in the model:\n",
    "- Use the SGD algorithm to train the model. The optimizers parameters can be included as variables for the function (lr, momentum and nesterov), see keras.optimizers.SGD.\n",
    "- For kernel regularization we will use an L2 regularizer with 1e-4 penalty term (see keras.regularizers). This enforces some sparsity to the solution.\n",
    "\n",
    "Build the following Keras sequential model<br>\n",
    "Layer 1: Hidden layer 1 (see keras.layers.Dense)<br>\n",
    "Layer 2: Dropout layer (see keras.layers.Dropout)<br>\n",
    "Layer 3: Hidden layer 2<br>\n",
    "Layer 4: Output layer - think about the activation function to be used in the output layer.<br>\n",
    "**Compile the model with the specified loss function and optimizer, and return it!**<br>\n",
    "\n",
    "Make sure that the function returns the model, so that the following code would work:<br>\n",
    "model = model_build_function(params)<br>\n",
    "fit = model.fit(x = ..., y = ..., batch_size = ..., epochs = ...)<br>\n",
    "\n",
    "<ins>Handy link:<ins><br>\n",
    "https://keras.io/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u001b[32mFinished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def train(params):\n",
    "\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "\n",
    "    print ('Params testing: ', params)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['units1'], input_dim = x_train_array.shape[1], kernel_initializer=params['weight_init']))\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "\n",
    "    model.add(Dense(params['units2'], kernel_initializer=params['weight_init']))\n",
    "    model.add(Dropout(params['dropout2']))\n",
    "\n",
    "    model.add(Dense(24))\n",
    "    model.add(Activation(params['activation']))\n",
    "\n",
    "    sgd_optimizer = keras.optimizers.SGD(lr=params['learning_rate']/1000, decay=1e-7, momentum=params['momentum'], nesterov=params['nesterov'])\n",
    "\n",
    "    model.compile(loss = params['loss'], optimizer = sgd_optimizer, metrics = [\"mae\"])\n",
    "\n",
    "    model.fit(x_train_array, y_train_array, epochs=params['nb_epochs'], batch_size=params['batch_size'], verbose = 1, validation_data = (x_val_array, y_val_array))\n",
    "\n",
    "    preds  = model.predict(x_val_array, batch_size = params['batch_size'], verbose = 1)\n",
    "    acc = mean_absolute_error(y_val_array, preds)\n",
    "    print('MAE:', acc)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "print(''), print(colored('Finished','green'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the hyperparameter search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Define the hyperparameters:\n",
    "- Hidden nodes layer 1 and 2, which need to take integer values only. The types of parameters that are available can be found in the Hyperopt FMin wiki. A quantized uniform distribution could be used here. To limit the search space, the domain can be divided in steps of 5 nodes. For hidden layer one, search between 150 and 300 nodes per layer. For hidden layer two, search between 50 and 200 nodes per layer.\n",
    "- Dropout rate, which needs to take continuous values lower than 1. A unifor distribution can be used, search between 0 and 0.5.\n",
    "- Activation function for the hidden layers. This is a clear case of the 'choice' function in hyperopt. Try 'ReLu' and the 'LeakyReLu'. Optional: add a nested uniform distribution for the alpha parameter of the LeakyReLu.\n",
    "- Loss function. another 'choice' parameter. Try the RMSE and MAE.\n",
    "- Weight initialization. Use the choice-type parameter to try both 'RandomNormal' and 'RandomUniform' (see Keras Initializers doc).\n",
    "- Learning rate of the SGD, to easy things make it a quantized unfirom distribution between 1 and 20 in steps of 1 and divide this by 1000 in your loop.\n",
    "- Momentum, make this a uniform distribution between 0 and 0.5.\n",
    "- Nesterov, which is a Boolean that can be described using the choice function.\n",
    "- Epochs, which is a integer value between 100 and 300. Steps of 10 can be used.\n",
    "- Batch size, which can take an integer value from 50 to 200. Note: if the optimization crashes due to memory issues, reduce the batch size.\n",
    "\n",
    "(2) Define the search space:\n",
    "In HyperOpt, a search space is defined as a python dictionary with the hyperparameters. Like in the following example:\n",
    "```python\n",
    "    n1 = hp.quniform('Hidden nodes layer 1', 150, 300, 5)\n",
    "    n2 = hp.quniform('Hidden nodes layer 2', 50, 200, 5)\n",
    "    \n",
    "    search_space = {\n",
    "        'Hidden nodes layer 1': n1,\n",
    "        'Hidden nodes layer 2': n2\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<ins>Handy link:<ins><br>\n",
    "http://hyperopt.github.io/hyperopt/#documentation <br>\n",
    "https://github.com/hyperopt/hyperopt/wiki/FMin <br>\n",
    "https://keras.io/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\u001b[32mFinished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "search_space = {    'units1': hp.quniform('units1', 150, 300, 5),\n",
    "                    'units2': hp.quniform('units2', 50, 200, 5),\n",
    "                    'dropout1': hp.uniform('dropout1', 0, 0.5),\n",
    "                    'dropout2': hp.uniform('dropout2', 0, 0.5),\n",
    "                    'batch_size': hp.choice('batch_size', [100]),\n",
    "                    'nb_epochs':  hp.choice('nb_epochs', [150]),\n",
    "                    'activation': hp.choice('activation', ['relu', LeakyReLU(alpha=0.05)]),\n",
    "                    'loss': hp.choice('loss', ['mae', rmse]),\n",
    "                    'weight_init': hp.choice('weight_init', ['random_normal', 'random_uniform']),\n",
    "                    'momentum': hp.uniform('momentum', 0, 0.5),\n",
    "                    'learning_rate': hp.quniform('learning_rate', 1, 20, 1),\n",
    "                    'nesterov': hp.choice('nesterov', [True, False]),\n",
    "                    }\n",
    "\n",
    "print(''), print(colored('Finished','green'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your train function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can build the last piece of the puzzle needed to optimize hyperparameters of your MLP.<br>\n",
    "\n",
    "Define a function that takes a dictionary of hyperparameters as input. Make sure to redefine integer values as such, since hyperopt returns floats from quantized distributions.\n",
    "\n",
    "The function should\n",
    "(1) Read the hyperparameters.\n",
    "(2) Loop over a 5-Fold Cross Validation (see scikit-learn KFold function) in which:\n",
    "- A Keras model is declared with given hyperparameters.\n",
    "- Scale the input-features using the scikitlearn StandardScaler. Scale the test-set with the scaling factors from the training set. This has to be done in the KFold loop to prevent information leakage.\n",
    "- The model is trained over the train set in the given fold.\n",
    "- The trained model is evaluated over the test set of the given fold, using the <ins>Mean Absolute Error!</ins> <br>\n",
    "note: you can read in your data before calling the function, this saves you a lot of runtime. \n",
    "(3) Make a python list with the MAE (for example called 'losses') of the five folds and return a dictionary in the following format:\n",
    "```python\n",
    "    {'loss': np.mean(losses), 'status': STATUS_OK, 'losses': losses}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Read in your data, no need to scale them. Just make sure to have an input features array (X) and a target array (y).<br>\n",
    "(2) Declare a hyperopt trials object.\n",
    "(3) Run the search! Let's use the Tree Parzen Estimator algorithm. Use the fmin function: \n",
    "```python \n",
    "def train(hyperparameters):\n",
    "    ...\n",
    "    return dict\n",
    "\n",
    "search_space = {...}\n",
    "trials = Trials()\n",
    "X, y = load_data()\n",
    "\n",
    "best = fmin(fn = train, \n",
    "            space = search_space, \n",
    "            algo = tpe.suggest, \n",
    "            max_evals = 500, \n",
    "            trials = trials, \n",
    "            show_progressbar = True\n",
    "           )\n",
    "```\n",
    "(4) Save your trials object! This can be stored as a pickle. Don't mess with pickles, since they can potentially form safety hazards for your PC. Here is an example of proper pickle-usage:\n",
    "```python\n",
    "save_trials_path = Path(path_to_folder)\n",
    "with open(save_trials_path / 'trials.pickle', 'wb') as pickle_file:\n",
    "    pickle.dump(trials, pickle_file)\n",
    "\n",
    "...rest of code\n",
    "```\n",
    "\n",
    "Note: run it once first with max_evals = 1 to check if everything works. Also, if this takes ages you can reduce the search space by having some hyperparameters fixed (for example by only using the MAE loss, fixing SGD parameters to the standard, using fixed epochs and/or batch_size), this would allow for a smaller amount of evals. This assignment is just to show what is possible on a big computer, this might not be feasible on your own PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\u001b[32mFinished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def loop():\n",
    "\n",
    "    trials = Trials()\n",
    "\n",
    "    best = fmin(    fn = train,\n",
    "                    space = search_space,\n",
    "                    algo = tpe.suggest,\n",
    "                    max_evals = 1,\n",
    "                    trials = trials,\n",
    "                    show_progressbar = True,\n",
    "                    )\n",
    "\n",
    "    save_trials_path = Path('./')\n",
    "    with open(save_trials_path / 'trials.pickle', 'wb') as pickle_file:\n",
    "        pickle.dump(trials, pickle_file)\n",
    "\n",
    "    losses.append(trials.losses())\n",
    "\n",
    "    return best, {'loss': np.mean(losses), 'status': STATUS_OK, 'losses': losses}\n",
    "    \n",
    "print(''), print(colored('Finished','green'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===========================]\n",
      " - 0s 8ms/step - loss: 32.6501 - mae: 28.7599 - val_loss: 29.9669 - val_mae: 25.7028\n",
      "\n",
      "Epoch 87/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 30.8777 - mae: 26.8438\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 32.5513 - mae: 28.6933\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 32.4909 - mae: 28.6435 - val_loss: 30.1544 - val_mae: 26.0470\n",
      "\n",
      "Epoch 88/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 32.3031 - mae: 28.6932\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 32.1792 - mae: 28.3688 - val_loss: 30.1627 - val_mae: 26.1230\n",
      "\n",
      "Epoch 89/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 32.4863 - mae: 28.8049\n",
      "\b\n",
      "11/15 [=====================>........]\n",
      " - ETA: 0s - loss: 31.8655 - mae: 28.1116\n",
      "\b\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 31.7166 - mae: 27.9392\n",
      "\b\n",
      "13/15 [=========================>....]\n",
      " - ETA: 0s - loss: 31.7583 - mae: 27.9787\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 18ms/step - loss: 31.7465 - mae: 27.9576 - val_loss: 30.6627 - val_mae: 26.9792\n",
      "\n",
      "Epoch 90/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 31.0270 - mae: 27.2503\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 31.2623 - mae: 27.4497 - val_loss: 30.9889 - val_mae: 27.5602\n",
      "\n",
      "Epoch 91/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 29.7657 - mae: 26.3242\n",
      "\b\n",
      "13/15 [=========================>....]\n",
      " - ETA: 0s - loss: 30.5971 - mae: 26.7759\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 30.6425 - mae: 26.8256 - val_loss: 31.1922 - val_mae: 27.9681\n",
      "\n",
      "Epoch 92/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 30.4902 - mae: 26.9356\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 30.1397 - mae: 26.4051\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 7ms/step - loss: 30.0896 - mae: 26.3566 - val_loss: 31.2017 - val_mae: 28.1219\n",
      "\n",
      "Epoch 93/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 30.2604 - mae: 26.4902\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 29.7557 - mae: 26.0558 - val_loss: 30.3627 - val_mae: 27.1282\n",
      "\n",
      "Epoch 94/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 29.1310 - mae: 25.5204\n",
      "\b\n",
      " 4/15 [=======>......................]\n",
      " - ETA: 0s - loss: 29.9023 - mae: 26.1919\n",
      "\b\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 29.6377 - mae: 26.0122\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 12ms/step - loss: 29.6383 - mae: 26.0179 - val_loss: 30.3496 - val_mae: 27.1534\n",
      "\n",
      "Epoch 95/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 29.6350 - mae: 25.9798\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 29.2545 - mae: 25.5928 - val_loss: 29.7975 - val_mae: 26.5066\n",
      "\n",
      "Epoch 96/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 28.8655 - mae: 24.9881\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 29.1024 - mae: 25.4181\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 29.1024 - mae: 25.4181 - val_loss: 30.0437 - val_mae: 26.8641\n",
      "\n",
      "Epoch 97/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 28.6153 - mae: 25.0229\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 28.8625 - mae: 25.1992 - val_loss: 29.5546 - val_mae: 26.2424\n",
      "\n",
      "Epoch 98/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 27.9540 - mae: 24.1549\n",
      "\b\n",
      "13/15 [=========================>....]\n",
      " - ETA: 0s - loss: 28.5887 - mae: 24.8803\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 9ms/step - loss: 28.5791 - mae: 24.8686 - val_loss: 28.4247 - val_mae: 24.9057\n",
      "\n",
      "Epoch 99/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 28.3532 - mae: 24.8821\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 28.3019 - mae: 24.6626 - val_loss: 27.1859 - val_mae: 23.2750\n",
      "\n",
      "Epoch 100/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 27.9498 - mae: 24.1677\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 27.9081 - mae: 24.2777\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 27.9081 - mae: 24.2777 - val_loss: 26.1580 - val_mae: 21.8652\n",
      "\n",
      "Epoch 101/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 26.6289 - mae: 23.3233\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 27.3471 - mae: 23.9220\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 27.3997 - mae: 23.9815 - val_loss: 26.0704 - val_mae: 21.3917\n",
      "\n",
      "Epoch 102/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 27.4225 - mae: 23.3898\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 26.6209 - mae: 23.1486\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 10ms/step - loss: 26.6209 - mae: 23.1486 - val_loss: 25.2513 - val_mae: 21.2347\n",
      "\n",
      "Epoch 103/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 25.5073 - mae: 22.1850\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 25.0085 - mae: 21.4919 - val_loss: 24.5890 - val_mae: 20.4459\n",
      "\n",
      "Epoch 104/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 23.7502 - mae: 20.3615\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 23.7194 - mae: 19.9404\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 23.7287 - mae: 19.9415 - val_loss: 24.1513 - val_mae: 19.7588\n",
      "\n",
      "Epoch 105/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 24.0086 - mae: 19.9666\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 22.9632 - mae: 18.9348\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 7ms/step - loss: 22.9632 - mae: 18.9348 - val_loss: 23.3213 - val_mae: 18.3029\n",
      "\n",
      "Epoch 106/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 21.2365 - mae: 17.2582\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 22.1734 - mae: 17.9411 - val_loss: 23.3467 - val_mae: 18.1664\n",
      "\n",
      "Epoch 107/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 21.5254 - mae: 17.3859\n",
      "\b\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 21.6544 - mae: 17.3906\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 9ms/step - loss: 21.6971 - mae: 17.4302 - val_loss: 23.0498 - val_mae: 17.7087\n",
      "\n",
      "Epoch 108/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 22.0228 - mae: 17.4463\n",
      "\b\n",
      "11/15 [=====================>........]\n",
      " - ETA: 0s - loss: 21.1361 - mae: 16.8831\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 21.2125 - mae: 16.9631 - val_loss: 22.7214 - val_mae: 17.3165\n",
      "\n",
      "Epoch 109/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 21.3303 - mae: 16.9220\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 20.8243 - mae: 16.5326 - val_loss: 22.6604 - val_mae: 17.2400\n",
      "\n",
      "Epoch 110/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 20.0448 - mae: 16.0620\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 20.3224 - mae: 16.0918 - val_loss: 22.3401 - val_mae: 16.9721\n",
      "\n",
      "Epoch 111/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 19.3141 - mae: 15.2913\n",
      "\b\n",
      "13/15 [=========================>....]\n",
      " - ETA: 0s - loss: 19.7929 - mae: 15.5959\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 19.6460 - mae: 15.4817 - val_loss: 22.4400 - val_mae: 17.4328\n",
      "\n",
      "Epoch 112/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 19.1188 - mae: 14.9302\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 19.0017 - mae: 14.9807\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 19.0166 - mae: 14.9873 - val_loss: 22.0702 - val_mae: 16.7521\n",
      "\n",
      "Epoch 113/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 18.7579 - mae: 14.7781\n",
      "\b\n",
      "11/15 [=====================>........]\n",
      " - ETA: 0s - loss: 17.9093 - mae: 14.2196\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 17.7876 - mae: 14.1113 - val_loss: 21.5428 - val_mae: 16.3565\n",
      "\n",
      "Epoch 114/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 16.9216 - mae: 13.6315\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 15.0807 - mae: 12.1204 - val_loss: 17.9521 - val_mae: 14.3567\n",
      "\n",
      "Epoch 115/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 13.4322 - mae: 11.0415\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 13.3407 - mae: 11.1628\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 7ms/step - loss: 13.3407 - mae: 11.1628 - val_loss: 12.9156 - val_mae: 11.1534\n",
      "\n",
      "Epoch 116/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 14.7591 - mae: 12.9363\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 12.8860 - mae: 10.9894\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 12.8564 - mae: 10.9662 - val_loss: 10.2838 - val_mae: 8.6465\n",
      "\n",
      "Epoch 117/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 11.4931 - mae: 9.8834\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 12.5236 - mae: 10.7426 - val_loss: 9.1348 - val_mae: 7.7408\n",
      "\n",
      "Epoch 118/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 11.6655 - mae: 10.0403\n",
      "\b\n",
      " 4/15 [=======>......................]\n",
      " - ETA: 0s - loss: 11.1172 - mae: 9.3306 \n",
      "\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 11.6924 - mae: 9.9167\n",
      "\n",
      "13/15 [=========================>....]\n",
      " - ETA: 0s - loss: 11.5554 - mae: 9.7929\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 16ms/step - loss: 11.5279 - mae: 9.7753 - val_loss: 8.8394 - val_mae: 7.4277\n",
      "\n",
      "Epoch 119/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 11.2588 - mae: 9.2176\n",
      "\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 11.3732 - mae: 9.6420\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 9ms/step - loss: 11.3491 - mae: 9.6307 - val_loss: 7.0241 - val_mae: 5.7124\n",
      "\n",
      "Epoch 120/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.9707 - mae: 8.0767\n",
      "\b\n",
      "11/15 [=====================>........]\n",
      " - ETA: 0s - loss: 10.3818 - mae: 8.7185\n",
      "\n",
      "13/15 [=========================>....]\n",
      " - ETA: 0s - loss: 10.4155 - mae: 8.7206\n",
      "\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 10.3283 - mae: 8.6517\n",
      "\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 10.3341 - mae: 8.6604\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 27ms/step - loss: 10.3341 - mae: 8.6604 - val_loss: 8.0091 - val_mae: 6.6629\n",
      "\n",
      "Epoch 121/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.7032 - mae: 7.9580\n",
      "\b\n",
      " 8/15 [===============>..............]\n",
      " - ETA: 0s - loss: 10.8083 - mae: 9.1451\n",
      "\n",
      " 9/15 [=================>............]\n",
      " - ETA: 0s - loss: 11.0573 - mae: 9.4016\n",
      "\n",
      "10/15 [===================>..........]\n",
      " - ETA: 0s - loss: 11.2732 - mae: 9.6370\n",
      "\n",
      "11/15 [=====================>........]\n",
      " - ETA: 0s - loss: 11.0744 - mae: 9.4442\n",
      "\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 10.9747 - mae: 9.3240\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 33ms/step - loss: 10.9747 - mae: 9.3240 - val_loss: 8.5693 - val_mae: 7.3033\n",
      "\n",
      "Epoch 122/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 12.4989 - mae: 10.7482\n",
      "  0%|          | 0/1 [00:21<?, ?trial/s, best loss=?]WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.100445). Check your callbacks.\n",
      "\b\n",
      " 2/15 [===>..........................]\n",
      " - ETA: 0s - loss: 11.9645 - mae: 10.3660\n",
      "\b\n",
      " 3/15 [=====>........................]\n",
      " - ETA: 0s - loss: 11.2191 - mae: 9.5780 \n",
      "\n",
      " 4/15 [=======>......................]\n",
      " - ETA: 0s - loss: 10.7660 - mae: 9.1430\n",
      "\n",
      " 8/15 [===============>..............]\n",
      " - ETA: 0s - loss: 10.6258 - mae: 8.9999\n",
      "\n",
      " 9/15 [=================>............]\n",
      " - ETA: 0s - loss: 10.5658 - mae: 8.9238\n",
      "\n",
      "10/15 [===================>..........]\n",
      " - ETA: 0s - loss: 10.6031 - mae: 8.9513\n",
      "\n",
      "15/15 [==============================]\n",
      " - 1s 36ms/step - loss: 10.5000 - mae: 8.8606 - val_loss: 10.3358 - val_mae: 9.0465\n",
      "\n",
      "Epoch 123/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 13.5868 - mae: 11.8922\n",
      "\b\n",
      "10/15 [===================>..........]\n",
      " - ETA: 0s - loss: 10.5373 - mae: 8.8897 \n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 10.2235 - mae: 8.5808 - val_loss: 7.1887 - val_mae: 5.8919\n",
      "\n",
      "Epoch 124/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 10.9075 - mae: 9.0478\n",
      "\n",
      " 2/15 [===>..........................]\n",
      " - ETA: 0s - loss: 10.8035 - mae: 8.9507\n",
      "\n",
      " 3/15 [=====>........................]\n",
      " - ETA: 0s - loss: 10.3989 - mae: 8.6305\n",
      "\n",
      " 4/15 [=======>......................]\n",
      " - ETA: 0s - loss: 10.2438 - mae: 8.5070\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 19ms/step - loss: 10.5500 - mae: 8.9411 - val_loss: 9.3447 - val_mae: 8.0470\n",
      "\n",
      "Epoch 125/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 11.2568 - mae: 9.8339\n",
      "\n",
      " 7/15 [=============>................]\n",
      " - ETA: 0s - loss: 10.3068 - mae: 8.6443\n",
      "\n",
      " 8/15 [===============>..............]\n",
      " - ETA: 0s - loss: 10.1699 - mae: 8.5070\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 12ms/step - loss: 9.8957 - mae: 8.2760 - val_loss: 7.0232 - val_mae: 5.7828\n",
      "\n",
      "Epoch 126/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 10.0928 - mae: 8.7027\n",
      "\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 10.3916 - mae: 8.8120\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 10.3916 - mae: 8.8120 - val_loss: 9.2648 - val_mae: 7.9744\n",
      "\n",
      "Epoch 127/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 11.8099 - mae: 10.2719\n",
      "\b\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 10.4805 - mae: 8.9040 \n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 9ms/step - loss: 10.4245 - mae: 8.8370 - val_loss: 7.0890 - val_mae: 5.7643\n",
      "\n",
      "Epoch 128/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.8164 - mae: 8.1288\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 9.6118 - mae: 8.0274 - val_loss: 7.5154 - val_mae: 6.2513\n",
      "\n",
      "Epoch 129/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 10.1067 - mae: 8.4743\n",
      "\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 9.3650 - mae: 7.7572 \n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 9.5293 - mae: 7.9359 - val_loss: 7.3928 - val_mae: 6.1763\n",
      "\n",
      "Epoch 130/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 10.6891 - mae: 8.9665\n",
      "\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 10.0336 - mae: 8.4510\n",
      "\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 9.8949 - mae: 8.3082 - val_loss: 9.0645 - val_mae: 7.7658\n",
      "\n",
      "Epoch 131/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 11.0561 - mae: 9.5241\n",
      "\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 9.9514 - mae: 8.3671 \n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 9ms/step - loss: 9.9514 - mae: 8.3671 - val_loss: 6.6355 - val_mae: 5.4191\n",
      "\n",
      "Epoch 132/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 8.6265 - mae: 7.0417\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 9.3866 - mae: 7.8087 - val_loss: 9.1963 - val_mae: 7.9350\n",
      "\n",
      "Epoch 133/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 13.5058 - mae: 11.9067\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 9.2193 - mae: 7.6411  \n",
      "\b\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 9.1767 - mae: 7.6037\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 14ms/step - loss: 9.1767 - mae: 7.6037 - val_loss: 7.6494 - val_mae: 6.4862\n",
      "\n",
      "Epoch 134/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.8044 - mae: 8.3873\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 9.5504 - mae: 7.9763\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 9ms/step - loss: 9.5224 - mae: 7.9462 - val_loss: 9.2156 - val_mae: 7.9317\n",
      "\n",
      "Epoch 135/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 10.2560 - mae: 8.8522\n",
      "\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 9.2739 - mae: 7.7308 \n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 9ms/step - loss: 9.2608 - mae: 7.7175 - val_loss: 6.6594 - val_mae: 5.4485\n",
      "\n",
      "Epoch 136/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.3490 - mae: 7.6659\n",
      "\b\n",
      "13/15 [=========================>....]\n",
      " - ETA: 0s - loss: 9.0039 - mae: 7.4750\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 10ms/step - loss: 8.9746 - mae: 7.4433 - val_loss: 6.5966 - val_mae: 5.3466\n",
      "\n",
      "Epoch 137/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.1320 - mae: 7.4011\n",
      "\b\n",
      "11/15 [=====================>........]\n",
      " - ETA: 0s - loss: 9.1838 - mae: 7.6352\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 25ms/step - loss: 9.0847 - mae: 7.5399 - val_loss: 7.4567 - val_mae: 6.1548\n",
      "\n",
      "Epoch 138/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.2861 - mae: 7.6978\n",
      "\b\n",
      "14/15 [===========================>..]\n",
      " - ETA: 0s - loss: 9.2102 - mae: 7.6675\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 9.2256 - mae: 7.6871 - val_loss: 6.8818 - val_mae: 5.6846\n",
      "\n",
      "Epoch 139/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.4252 - mae: 7.9019\n",
      "\b\n",
      "10/15 [===================>..........]\n",
      " - ETA: 0s - loss: 9.1633 - mae: 7.6191\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 9.0557 - mae: 7.5074 - val_loss: 6.9924 - val_mae: 5.7944\n",
      "\n",
      "Epoch 140/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.3685 - mae: 8.0152\n",
      "\b\n",
      " 2/15 [===>..........................]\n",
      " - ETA: 0s - loss: 9.6301 - mae: 8.0562\n",
      "\b\n",
      "10/15 [===================>..........]\n",
      " - ETA: 0s - loss: 9.3770 - mae: 7.8172\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 13ms/step - loss: 9.2846 - mae: 7.7169 - val_loss: 8.6269 - val_mae: 7.3361\n",
      "\n",
      "Epoch 141/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.6423 - mae: 7.9611\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 9.0998 - mae: 7.5519 - val_loss: 6.8145 - val_mae: 5.5770\n",
      "\n",
      "Epoch 142/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 8.1349 - mae: 6.7030\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 9.2019 - mae: 7.6590 - val_loss: 7.1340 - val_mae: 5.8423\n",
      "\n",
      "Epoch 143/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 8.0577 - mae: 6.6632\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 8.9368 - mae: 7.3908 - val_loss: 6.2051 - val_mae: 4.9734\n",
      "\n",
      "Epoch 144/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 8.3116 - mae: 6.6590\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 5ms/step - loss: 8.6179 - mae: 7.0830 - val_loss: 6.3659 - val_mae: 5.1226\n",
      "\n",
      "Epoch 145/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 8.9135 - mae: 7.2659\n",
      "\b\n",
      "12/15 [=======================>......]\n",
      " - ETA: 0s - loss: 8.5155 - mae: 6.9705\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 10ms/step - loss: 8.5952 - mae: 7.0634 - val_loss: 6.5100 - val_mae: 5.3181\n",
      "\n",
      "Epoch 146/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.4349 - mae: 7.8456\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 9.0177 - mae: 7.4973\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 9.0177 - mae: 7.4973 - val_loss: 6.2905 - val_mae: 5.0340\n",
      "\n",
      "Epoch 147/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 7.7684 - mae: 6.2766\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - ETA: 0s - loss: 8.3723 - mae: 6.8454\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 8.3723 - mae: 6.8454 - val_loss: 6.3574 - val_mae: 5.0956\n",
      "\n",
      "Epoch 148/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 8.3590 - mae: 6.7757\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 6ms/step - loss: 8.5154 - mae: 7.0076 - val_loss: 7.3219 - val_mae: 6.1058\n",
      "\n",
      "Epoch 149/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 10.5075 - mae: 9.0085\n",
      "\n",
      "10/15 [===================>..........]\n",
      " - ETA: 0s - loss: 8.6109 - mae: 7.1231 \n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 8.5208 - mae: 7.0178 - val_loss: 6.5818 - val_mae: 5.4091\n",
      "\n",
      "Epoch 150/150\n",
      " 1/15 [=>............................]\n",
      " - ETA: 0s - loss: 9.0648 - mae: 7.3933\n",
      "\b\n",
      " 6/15 [===========>..................]\n",
      " - ETA: 0s - loss: 8.2639 - mae: 6.7538\n",
      "\b\n",
      "15/15 [==============================]\n",
      " - 0s 8ms/step - loss: 8.4495 - mae: 6.9508 - val_loss: 6.3226 - val_mae: 5.0686\n",
      "\n",
      "1/4 [======>.......................]\n",
      " - ETA: 0s\n",
      "\n",
      "4/4 [==============================]\n",
      " - 0s 8ms/step\n",
      "\n",
      "MAE:\n",
      "5.068568061776802\n",
      "100%|██████████| 1/1 [00:26<00:00, 26.79s/trial, best loss: -5.068568061776802]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "X = pd.read_pickle(f\"./features/feat_X_notscaled.pkl\")\n",
    "y = pd.read_pickle(f\"./features/feat_y_notscaled.pkl\")\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "losses = []\n",
    "\n",
    "cv = KFold(n_splits = 5, random_state = 42, shuffle = False)\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    def scale_data(array, means = scaler.mean_, stds = scaler.var_**0.5):\n",
    "        return (array-means)/stds\n",
    "\n",
    "    X_train = scale_data(X_train)\n",
    "    X_test = scale_data(X_test)\n",
    "\n",
    "    x_train_array = np.array(X_train, dtype = float)\n",
    "    y_train_array = np.array(y_train)\n",
    "    x_val_array = np.array(X_test, dtype = float)\n",
    "    y_val_array = np.array(y_test)\n",
    "\n",
    "    # Run loop\n",
    "\n",
    "    best, loss = loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34mBest:\u001b[0m\n\n\u001b[34m{'activation': 0, 'batch_size': 0, 'dropout1': 0.04922713379565635, 'dropout2': 0.4567421554223234, 'learning_rate': 10.0, 'loss': 1, 'momentum': 0.31131457121168254, 'nb_epochs': 0, 'nesterov': 0, 'units1': 245.0, 'units2': 150.0, 'weight_init': 1}\u001b[0m\n\n\u001b[34mLoss:\u001b[0m\n\n\u001b[34m{'loss': -5.508870356619624, 'status': 'ok', 'losses': [[-5.4181375410799815], [-4.50246074013583], [-5.375756858338774], [-7.179428581766737], [-5.068568061776802]]}\u001b[0m\n\n\u001b[32mFinished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(colored(f'Best:','blue')), print(''), print(colored(f'{best}','blue'))\n",
    "\n",
    "print(''), print(colored(f'Loss:','blue')), print(''), print(colored(f'{loss}','blue'))\n",
    "\n",
    "print(''), print(colored('Finished','green'));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}