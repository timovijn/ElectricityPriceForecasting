{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + Tensorflow and Hypteropt Python tutorial\n",
    "Made by Ties van der Heijden, TU Delft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will contintue with Dutch DAM price forecasting. This time we will give a detailed specification of our Neural Network, and we will optimize hyperparameters using HyperOpt.\n",
    "\n",
    "To do this, the following packages are necessary:\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Matplotlib\n",
    "- Tensorflow\n",
    "\n",
    "And some specific functions are handy:\n",
    "- SciKit Learn: KFold, StandardScaler\n",
    "- Pathlib: Path\n",
    "\n",
    "\n",
    "PS: Be sure to create a new environment for TF + Keras + Hyperopt, since pip and anaconda don't work too well together and can cause errors in the future. Better to have them conflict in a new python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model creation - MLP\n",
    "\n",
    "Start the function with the following command:\n",
    "```python\n",
    "tensorflow.keras.backend.clear_session()\n",
    "```\n",
    "Else Keras will keep all the trained models stored in the RAM, which causes the memory to slowly fill up.<br>\n",
    "\n",
    "First we will build a function that returns a keras MLP as a function of its hyperparameters. Use the following hyperparameters:\n",
    "- Hidden nodes in layer 1\n",
    "- Hidden nodes in layer 2\n",
    "- Activation function of the hidden layers\n",
    "- Loss function (see keras.losses)\n",
    "- Dropout rate\n",
    "- Weight initialization (see keras.initializers)\n",
    "\n",
    "We will fix some things in the model:\n",
    "- Use the SGD algorithm to train the model. The optimizers parameters can be included as variables for the function (lr, momentum and nesterov), see keras.optimizers.SGD.\n",
    "- For kernel regularization we will use an L2 regularizer with 1e-4 penalty term (see keras.regularizers). This enforces some sparsity to the solution.\n",
    "\n",
    "Build the following Keras sequential model<br>\n",
    "Layer 1: Hidden layer 1 (see keras.layers.Dense)<br>\n",
    "Layer 2: Dropout layer (see keras.layers.Dropout)<br>\n",
    "Layer 3: Hidden layer 2<br>\n",
    "Layer 4: Output layer - think about the activation function to be used in the output layer.<br>\n",
    "**Compile the model with the specified loss function and optimizer, and return it!**<br>\n",
    "\n",
    "Make sure that the function returns the model, so that the following code would work:<br>\n",
    "model = model_build_function(params)<br>\n",
    "fit = model.fit(x = ..., y = ..., batch_size = ..., epochs = ...)<br>\n",
    "\n",
    "<ins>Handy link:<ins><br>\n",
    "https://keras.io/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(params):\n",
    "\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "\n",
    "    print ('Params testing: ', params)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(params['units1'], input_dim = x_train_array.shape[1], kernel_initializer=params['weight_init']))\n",
    "    # model.add(PReLU())\n",
    "    model.add(Dropout(params['dropout1']))\n",
    "\n",
    "    model.add(Dense(params['units2'], kernel_initializer=params['weight_init']))\n",
    "    # model.add(PReLU())\n",
    "    model.add(Dropout(params['dropout2']))\n",
    "\n",
    "    model.add(Dense(24))\n",
    "    model.add(Activation(params['activation']))\n",
    "\n",
    "    sgd_optimizer = keras.optimizers.SGD(lr=params['learning_rate']/1000, decay=1e-7, momentum=params['momentum'], nesterov=params['nesterov'])\n",
    "\n",
    "    model.compile(loss = params['loss'], optimizer = sgd_optimizer, metrics = [\"mae\"])\n",
    "\n",
    "    model.fit(x_train_array, y_train_array, epochs=params['nb_epochs'], batch_size=params['batch_size'], verbose = 1, validation_data = (x_val_array, y_val_array))\n",
    "\n",
    "    preds  = model.predict(x_val_array, batch_size = params['batch_size'], verbose = 1)\n",
    "    acc = mean_absolute_error(y_val_array, preds)\n",
    "    print('MAE:', acc)\n",
    "    sys.stdout.flush()\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the hyperparameter search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Define the hyperparameters:\n",
    "- Hidden nodes layer 1 and 2, which need to take integer values only. The types of parameters that are available can be found in the Hyperopt FMin wiki. A quantized uniform distribution could be used here. To limit the search space, the domain can be divided in steps of 5 nodes. For hidden layer one, search between 150 and 300 nodes per layer. For hidden layer two, search between 50 and 200 nodes per layer.\n",
    "- Dropout rate, which needs to take continuous values lower than 1. A unifor distribution can be used, search between 0 and 0.5.\n",
    "- Activation function for the hidden layers. This is a clear case of the 'choice' function in hyperopt. Try 'ReLu' and the 'LeakyReLu'. Optional: add a nested uniform distribution for the alpha parameter of the LeakyReLu.\n",
    "- Loss function. another 'choice' parameter. Try the RMSE and MAE.\n",
    "- Weight initialization. Use the choice-type parameter to try both 'RandomNormal' and 'RandomUniform' (see Keras Initializers doc).\n",
    "- Learning rate of the SGD, to easy things make it a quantized unfirom distribution between 1 and 20 in steps of 1 and divide this by 1000 in your loop.\n",
    "- Momentum, make this a uniform distribution between 0 and 0.5.\n",
    "- Nesterov, which is a Boolean that can be described using the choice function.\n",
    "- Epochs, which is a integer value between 100 and 300. Steps of 10 can be used.\n",
    "- Batch size, which can take an integer value from 50 to 200. Note: if the optimization crashes due to memory issues, reduce the batch size.\n",
    "\n",
    "(2) Define the search space:\n",
    "In HyperOpt, a search space is defined as a python dictionary with the hyperparameters. Like in the following example:\n",
    "```python\n",
    "    n1 = hp.quniform('Hidden nodes layer 1', 150, 300, 5)\n",
    "    n2 = hp.quniform('Hidden nodes layer 2', 50, 200, 5)\n",
    "    \n",
    "    search_space = {\n",
    "        'Hidden nodes layer 1': n1,\n",
    "        'Hidden nodes layer 2': n2\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<ins>Handy link:<ins><br>\n",
    "http://hyperopt.github.io/hyperopt/#documentation <br>\n",
    "https://github.com/hyperopt/hyperopt/wiki/FMin <br>\n",
    "https://keras.io/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = { 'choice': hp.choice('layers_number',\n",
    "                             [{'layers': 'two'},\n",
    "                             {'layers': 'three',\n",
    "                             'units3': hp.choice('units3', [32, 64, 256]),\n",
    "                             'dropout3': hp.choice('dropout3', np.linspace(0.1, 0.3, 3, dtype=float))\n",
    "                             }]),\n",
    "\n",
    "            'units1': hp.quniform('units1', 150, 300, 5),\n",
    "            # 'units1': hp.choice('units1', [150, 768, 1024]),\n",
    "\n",
    "            'units2': hp.quniform('units2', 50, 200, 5),\n",
    "            # 'units2': hp.choice('units2', [128, 256, 512]),\n",
    "            #'units3': hp.choice('units3', [32, 64, 256]),\n",
    "\n",
    "            'dropout1': hp.uniform('dropout1', 0, 0.5),\n",
    "            # 'dropout1': hp.choice('dropout1', np.linspace(0.3, 0.5, 3, dtype=float)),\n",
    "\n",
    "            'dropout2': hp.uniform('dropout2', 0, 0.5),\n",
    "            # 'dropout2': hp.choice('dropout2', np.linspace(0.1, 0.3, 3, dtype=float)),\n",
    "\n",
    "            # 'dropout3': hp.uniform('dropout3', 0, 0.5),\n",
    "            #'dropout3': hp.choice('dropout3', np.linspace(0.1, 0.3, 3, dtype=float)),\n",
    "\n",
    "            'batch_size' : hp.choice('batch_size', [100]),\n",
    "\n",
    "            'nb_epochs' :  hp.choice('nb_epochs', [100, 300, 10]),\n",
    "\n",
    "            'activation': hp.choice('activation', ['relu', LeakyReLU(alpha=0.05)]),\n",
    "            \n",
    "            'loss': hp.choice('loss', ['mae', 'rmse']),\n",
    "\n",
    "            'weight_init': hp.choice('weight_init', ['random_normal', 'random_uniform']),\n",
    "\n",
    "            'momentum': hp.uniform('momentum', 0, 0.5),\n",
    "\n",
    "            'learning_rate': hp.quniform('learning_rate', 1, 20, 1),\n",
    "\n",
    "            'nesterov': hp.choice('nesterov', [True, False]),\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space = { 'choice': hp.choice('layers_number',\n",
    "#                              [{'layers': 'two'},\n",
    "#                              {'layers': 'three',\n",
    "#                              'units3': hp.choice('units3', [32, 64, 256]),\n",
    "#                              'dropout3': hp.choice('dropout3', np.linspace(0.1, 0.3, 3, dtype=float))\n",
    "#                              }]),\n",
    "\n",
    "#             'units1': hp.choice('units1', [150, 768, 1024]),\n",
    "#             'units2': hp.choice('units2', [128, 256, 512]),\n",
    "#             #'units3': hp.choice('units3', [32, 64, 256]), \n",
    "\n",
    "#             'dropout1': hp.choice('dropout1', np.linspace(0.3, 0.5, 3, dtype=float)),\n",
    "#             'dropout2': hp.choice('dropout2', np.linspace(0.1, 0.3, 3, dtype=float)),\n",
    "#             #'dropout3': hp.choice('dropout3', np.linspace(0.1, 0.3, 3, dtype=float)),\n",
    "\n",
    "#             'batch_size' : hp.choice('batch_size', [128, 256, 512]),\n",
    "\n",
    "#             'nb_epochs' :  hp.choice('nb_epochs', [30, 50, 100]),\n",
    "            \n",
    "#         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your train function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can build the last piece of the puzzle needed to optimize hyperparameters of your MLP.<br>\n",
    "\n",
    "Define a function that takes a dictionary of hyperparameters as input. Make sure to redefine integer values as such, since hyperopt returns floats from quantized distributions.\n",
    "\n",
    "The function should\n",
    "(1) Read the hyperparameters.\n",
    "(2) Loop over a 5-Fold Cross Validation (see scikit-learn KFold function) in which:\n",
    "- A Keras model is declared with given hyperparameters.\n",
    "- Scale the input-features using the scikitlearn StandardScaler. Scale the test-set with the scaling factors from the training set. This has to be done in the KFold loop to prevent information leakage.\n",
    "- The model is trained over the train set in the given fold.\n",
    "- The trained model is evaluated over the test set of the given fold, using the <ins>Mean Absolute Error!</ins> <br>\n",
    "note: you can read in your data before calling the function, this saves you a lot of runtime. \n",
    "(3) Make a python list with the MAE (for example called 'losses') of the five folds and return a dictionary in the following format:\n",
    "```python\n",
    "    {'loss': np.mean(losses), 'status': STATUS_OK, 'losses': losses}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Read in your data, no need to scale them. Just make sure to have an input features array (X) and a target array (y).<br>\n",
    "(2) Declare a hyperopt trials object.\n",
    "(3) Run the search! Let's use the Tree Parzen Estimator algorithm. Use the fmin function: \n",
    "```python \n",
    "def train(hyperparameters):\n",
    "    ...\n",
    "    return dict\n",
    "\n",
    "search_space = {...}\n",
    "trials = Trials()\n",
    "X, y = load_data()\n",
    "\n",
    "best = fmin(fn = train, \n",
    "            space = search_space, \n",
    "            algo = tpe.suggest, \n",
    "            max_evals = 500, \n",
    "            trials = trials, \n",
    "            show_progressbar = True\n",
    "           )\n",
    "```\n",
    "(4) Save your trials object! This can be stored as a pickle. Don't mess with pickles, since they can potentially form safety hazards for your PC. Here is an example of proper pickle-usage:\n",
    "```python\n",
    "save_trials_path = Path(path_to_folder)\n",
    "with open(save_trials_path / 'trials.pickle', 'wb') as pickle_file:\n",
    "    pickle.dump(trials, pickle_file)\n",
    "\n",
    "...rest of code\n",
    "```\n",
    "\n",
    "Note: run it once first with max_evals = 1 to check if everything works. Also, if this takes ages you can reduce the search space by having some hyperparameters fixed (for example by only using the MAE loss, fixing SGD parameters to the standard, using fixed epochs and/or batch_size), this would allow for a smaller amount of evals. This assignment is just to show what is possible on a big computer, this might not be feasible on your own PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==]\n",
      " - ETA: 0s - loss: 6.3831 - mae: 6.3831\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 18ms/step - loss: 6.3831 - mae: 6.3831 - val_loss: 4.5722 - val_mae: 4.5722\n",
      "\n",
      "Epoch 250/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4448 - mae: 6.4448\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.4343 - mae: 6.4343\n",
      "\b\n",
      "16/17 [===========================>..]\n",
      " - ETA: 0s - loss: 6.4274 - mae: 6.4274\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - ETA: 0s - loss: 6.4177 - mae: 6.4177\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 13ms/step - loss: 6.4177 - mae: 6.4177 - val_loss: 5.6303 - val_mae: 5.6303\n",
      "\n",
      "Epoch 251/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.3999 - mae: 6.3999\n",
      "\b\n",
      " 2/17 [==>...........................]\n",
      " - ETA: 0s - loss: 6.2833 - mae: 6.2833\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.3563 - mae: 6.3563\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 13ms/step - loss: 6.4082 - mae: 6.4082 - val_loss: 5.1943 - val_mae: 5.1943\n",
      "\n",
      "Epoch 252/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 7.0732 - mae: 7.0732\n",
      "\b\n",
      " 6/17 [=========>....................]\n",
      " - ETA: 0s - loss: 6.5438 - mae: 6.5438\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.4112 - mae: 6.4112\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 11ms/step - loss: 6.4217 - mae: 6.4217 - val_loss: 4.5485 - val_mae: 4.5485\n",
      "\n",
      "Epoch 253/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.9297 - mae: 6.9297\n",
      "\b\n",
      " 9/17 [==============>...............]\n",
      " - ETA: 0s - loss: 6.5050 - mae: 6.5050\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 9ms/step - loss: 6.4421 - mae: 6.4421 - val_loss: 4.7288 - val_mae: 4.7288\n",
      "\n",
      "Epoch 254/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4793 - mae: 6.4793\n",
      "\b\n",
      " 5/17 [=======>......................]\n",
      " - ETA: 0s - loss: 6.4263 - mae: 6.4263\n",
      "\b\n",
      " 8/17 [=============>................]\n",
      " - ETA: 0s - loss: 6.5096 - mae: 6.5096\n",
      "\b\n",
      "15/17 [=========================>....]\n",
      " - ETA: 0s - loss: 6.3797 - mae: 6.3797\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 17ms/step - loss: 6.3793 - mae: 6.3793 - val_loss: 5.0285 - val_mae: 5.0285\n",
      "\n",
      "Epoch 255/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 7.0761 - mae: 7.0761\n",
      "\b\n",
      "11/17 [==================>...........]\n",
      " - ETA: 0s - loss: 6.3714 - mae: 6.3714\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 9ms/step - loss: 6.3825 - mae: 6.3825 - val_loss: 4.7309 - val_mae: 4.7309\n",
      "\n",
      "Epoch 256/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.7429 - mae: 6.7429\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.4096 - mae: 6.4096\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - ETA: 0s - loss: 6.4914 - mae: 6.4914\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 12ms/step - loss: 6.4914 - mae: 6.4914 - val_loss: 5.0957 - val_mae: 5.0957\n",
      "\n",
      "Epoch 257/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.0688 - mae: 6.0688\n",
      "\b\n",
      " 2/17 [==>...........................]\n",
      " - ETA: 0s - loss: 6.1646 - mae: 6.1646\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - ETA: 0s - loss: 6.3842 - mae: 6.3842\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 12ms/step - loss: 6.3842 - mae: 6.3842 - val_loss: 4.8909 - val_mae: 4.8909\n",
      "\n",
      "Epoch 258/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 5.8611 - mae: 5.8611\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.3645 - mae: 6.3645\n",
      "\b\n",
      "16/17 [===========================>..]\n",
      " - ETA: 0s - loss: 6.2871 - mae: 6.2871\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 18ms/step - loss: 6.3103 - mae: 6.3103 - val_loss: 4.6918 - val_mae: 4.6918\n",
      "\n",
      "Epoch 259/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.0898 - mae: 6.0898\n",
      "\b\n",
      " 8/17 [=============>................]\n",
      " - ETA: 0s - loss: 6.1624 - mae: 6.1624\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.4566 - mae: 6.4566\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 11ms/step - loss: 6.4447 - mae: 6.4447 - val_loss: 5.4278 - val_mae: 5.4278\n",
      "\n",
      "Epoch 260/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 7.0062 - mae: 7.0062\n",
      "\b\n",
      "14/17 [=======================>......]\n",
      " - ETA: 0s - loss: 6.4525 - mae: 6.4525\n",
      "\b\n",
      "15/17 [=========================>....]\n",
      " - ETA: 0s - loss: 6.4459 - mae: 6.4459\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 11ms/step - loss: 6.4120 - mae: 6.4120 - val_loss: 4.9324 - val_mae: 4.9324\n",
      "\n",
      "Epoch 261/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 5.6626 - mae: 5.6626\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.3545 - mae: 6.3545\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.3537 - mae: 6.3537\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 11ms/step - loss: 6.4132 - mae: 6.4132 - val_loss: 4.8227 - val_mae: 4.8227\n",
      "\n",
      "Epoch 262/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4410 - mae: 6.4410\n",
      "\b\n",
      " 5/17 [=======>......................]\n",
      " - ETA: 0s - loss: 6.3669 - mae: 6.3669\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.3954 - mae: 6.3954\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 10ms/step - loss: 6.4080 - mae: 6.4080 - val_loss: 4.6556 - val_mae: 4.6556\n",
      "\n",
      "Epoch 263/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.2555 - mae: 6.2555\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.1655 - mae: 6.1655\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.2008 - mae: 6.2008\n",
      "\b\n",
      "14/17 [=======================>......]\n",
      " - ETA: 0s - loss: 6.2311 - mae: 6.2311\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 14ms/step - loss: 6.2843 - mae: 6.2843 - val_loss: 4.6429 - val_mae: 4.6429\n",
      "\n",
      "Epoch 264/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 5.8674 - mae: 5.8674\n",
      "\b\n",
      " 9/17 [==============>...............]\n",
      " - ETA: 0s - loss: 6.0735 - mae: 6.0735\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.0770 - mae: 6.0770\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 12ms/step - loss: 6.2689 - mae: 6.2689 - val_loss: 4.4530 - val_mae: 4.4530\n",
      "\n",
      "Epoch 265/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4496 - mae: 6.4496\n",
      "\b\n",
      "16/17 [===========================>..]\n",
      " - ETA: 0s - loss: 6.3642 - mae: 6.3642\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - ETA: 0s - loss: 6.3734 - mae: 6.3734\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 12ms/step - loss: 6.3734 - mae: 6.3734 - val_loss: 4.8286 - val_mae: 4.8286\n",
      "\n",
      "Epoch 266/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.6146 - mae: 6.6146\n",
      "\b\n",
      "14/17 [=======================>......]\n",
      " - ETA: 0s - loss: 6.4580 - mae: 6.4580\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - ETA: 0s - loss: 6.4499 - mae: 6.4499\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 10ms/step - loss: 6.4499 - mae: 6.4499 - val_loss: 4.5871 - val_mae: 4.5871\n",
      "\n",
      "Epoch 267/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 5.9512 - mae: 5.9512\n",
      "\b\n",
      " 2/17 [==>...........................]\n",
      " - ETA: 0s - loss: 6.1622 - mae: 6.1622\n",
      "\b\n",
      " 3/17 [====>.........................]\n",
      " - ETA: 0s - loss: 6.0866 - mae: 6.0866\n",
      "\b\n",
      " 7/17 [===========>..................]\n",
      " - ETA: 0s - loss: 6.4231 - mae: 6.4231\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 24ms/step - loss: 6.3013 - mae: 6.3013 - val_loss: 4.7403 - val_mae: 4.7403\n",
      "\n",
      "Epoch 268/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.3791 - mae: 6.3791\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.2625 - mae: 6.2625\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 12ms/step - loss: 6.2719 - mae: 6.2719 - val_loss: 4.9013 - val_mae: 4.9013\n",
      "\n",
      "Epoch 269/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.1443 - mae: 6.1443\n",
      "\b\n",
      "14/17 [=======================>......]\n",
      " - ETA: 0s - loss: 6.3191 - mae: 6.3191\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 11ms/step - loss: 6.2678 - mae: 6.2678 - val_loss: 4.8873 - val_mae: 4.8873\n",
      "\n",
      "Epoch 270/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.5840 - mae: 6.5840\n",
      "\b\n",
      "15/17 [=========================>....]\n",
      " - ETA: 0s - loss: 6.4023 - mae: 6.4023\n",
      "\b\n",
      "16/17 [===========================>..]\n",
      " - ETA: 0s - loss: 6.3678 - mae: 6.3678\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - ETA: 0s - loss: 6.3565 - mae: 6.3565\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 16ms/step - loss: 6.3565 - mae: 6.3565 - val_loss: 4.4835 - val_mae: 4.4835\n",
      "\n",
      "Epoch 271/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 5.4627 - mae: 5.4627\n",
      "\b\n",
      "11/17 [==================>...........]\n",
      " - ETA: 0s - loss: 6.3801 - mae: 6.3801\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.3630 - mae: 6.3630\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 15ms/step - loss: 6.3665 - mae: 6.3665 - val_loss: 4.5137 - val_mae: 4.5137\n",
      "\n",
      "Epoch 272/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4821 - mae: 6.4821\n",
      "\b\n",
      "14/17 [=======================>......]\n",
      " - ETA: 0s - loss: 6.2962 - mae: 6.2962\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 9ms/step - loss: 6.3238 - mae: 6.3238 - val_loss: 4.4784 - val_mae: 4.4784\n",
      "\n",
      "Epoch 273/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.0244 - mae: 6.0244\n",
      "\b\n",
      " 9/17 [==============>...............]\n",
      " - ETA: 0s - loss: 6.2491 - mae: 6.2491\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 12ms/step - loss: 6.3197 - mae: 6.3197 - val_loss: 4.5456 - val_mae: 4.5456\n",
      "\n",
      "Epoch 274/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4716 - mae: 6.4716\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.2994 - mae: 6.2994\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 15ms/step - loss: 6.3165 - mae: 6.3165 - val_loss: 4.9224 - val_mae: 4.9224\n",
      "\n",
      "Epoch 275/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.2439 - mae: 6.2439\n",
      "\b\n",
      "14/17 [=======================>......]\n",
      " - ETA: 0s - loss: 6.2243 - mae: 6.2243\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 8ms/step - loss: 6.2685 - mae: 6.2685 - val_loss: 4.5533 - val_mae: 4.5533\n",
      "\n",
      "Epoch 276/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.7173 - mae: 6.7173\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.2483 - mae: 6.2483\n",
      "\b\n",
      "11/17 [==================>...........]\n",
      " - ETA: 0s - loss: 6.2254 - mae: 6.2254\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 17ms/step - loss: 6.2869 - mae: 6.2869 - val_loss: 4.6895 - val_mae: 4.6895\n",
      "\n",
      "Epoch 277/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4399 - mae: 6.4399\n",
      "\b\n",
      " 9/17 [==============>...............]\n",
      " - ETA: 0s - loss: 6.2507 - mae: 6.2507\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 15ms/step - loss: 6.2484 - mae: 6.2484 - val_loss: 4.8954 - val_mae: 4.8954\n",
      "\n",
      "Epoch 278/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.8401 - mae: 6.8401\n",
      "\b\n",
      "14/17 [=======================>......]\n",
      " - ETA: 0s - loss: 6.2326 - mae: 6.2326\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 13ms/step - loss: 6.2511 - mae: 6.2511 - val_loss: 4.5479 - val_mae: 4.5479\n",
      "\n",
      "Epoch 279/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.2783 - mae: 6.2783\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.2678 - mae: 6.2678\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.2801 - mae: 6.2801\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 13ms/step - loss: 6.2929 - mae: 6.2929 - val_loss: 4.4737 - val_mae: 4.4737\n",
      "\n",
      "Epoch 280/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.3424 - mae: 6.3424\n",
      "\b\n",
      " 7/17 [===========>..................]\n",
      " - ETA: 0s - loss: 6.3586 - mae: 6.3586\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 8ms/step - loss: 6.2628 - mae: 6.2628 - val_loss: 4.5730 - val_mae: 4.5730\n",
      "\n",
      "Epoch 281/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4281 - mae: 6.4281\n",
      "\b\n",
      " 9/17 [==============>...............]\n",
      " - ETA: 0s - loss: 6.2013 - mae: 6.2013\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.1827 - mae: 6.1827\n",
      "\b\n",
      "11/17 [==================>...........]\n",
      " - ETA: 0s - loss: 6.1887 - mae: 6.1887\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 14ms/step - loss: 6.2776 - mae: 6.2776 - val_loss: 4.5878 - val_mae: 4.5878\n",
      "\n",
      "Epoch 282/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.1067 - mae: 6.1067\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.2663 - mae: 6.2663\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.2964 - mae: 6.2964\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 13ms/step - loss: 6.2761 - mae: 6.2761 - val_loss: 4.5235 - val_mae: 4.5235\n",
      "\n",
      "Epoch 283/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4968 - mae: 6.4968\n",
      "\b\n",
      "11/17 [==================>...........]\n",
      " - ETA: 0s - loss: 6.3862 - mae: 6.3862\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.4386 - mae: 6.4386\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 13ms/step - loss: 6.3830 - mae: 6.3830 - val_loss: 4.4829 - val_mae: 4.4829\n",
      "\n",
      "Epoch 284/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.1193 - mae: 6.1193\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.5088 - mae: 6.5088\n",
      "\b\n",
      "14/17 [=======================>......]\n",
      " - ETA: 0s - loss: 6.4975 - mae: 6.4975\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 12ms/step - loss: 6.5648 - mae: 6.5648 - val_loss: 4.4242 - val_mae: 4.4242\n",
      "\n",
      "Epoch 285/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 7.1457 - mae: 7.1457\n",
      "\b\n",
      " 8/17 [=============>................]\n",
      " - ETA: 0s - loss: 6.2818 - mae: 6.2818\n",
      "\b\n",
      "13/17 [=====================>........]\n",
      " - ETA: 0s - loss: 6.2779 - mae: 6.2779\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 11ms/step - loss: 6.2750 - mae: 6.2750 - val_loss: 4.7161 - val_mae: 4.7161\n",
      "\n",
      "Epoch 286/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4957 - mae: 6.4957\n",
      "\b\n",
      "15/17 [=========================>....]\n",
      " - ETA: 0s - loss: 6.3208 - mae: 6.3208\n",
      "\b\n",
      "16/17 [===========================>..]\n",
      " - ETA: 0s - loss: 6.2797 - mae: 6.2797\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 14ms/step - loss: 6.2814 - mae: 6.2814 - val_loss: 4.7259 - val_mae: 4.7259\n",
      "\n",
      "Epoch 287/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.6760 - mae: 6.6760\n",
      "\b\n",
      " 4/17 [======>.......................]\n",
      " - ETA: 0s - loss: 6.2190 - mae: 6.2190\n",
      "\b\n",
      " 9/17 [==============>...............]\n",
      " - ETA: 0s - loss: 6.2789 - mae: 6.2789\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 12ms/step - loss: 6.2801 - mae: 6.2801 - val_loss: 4.7691 - val_mae: 4.7691\n",
      "\n",
      "Epoch 288/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.2741 - mae: 6.2741\n",
      "\b\n",
      " 2/17 [==>...........................]\n",
      " - ETA: 0s - loss: 6.2681 - mae: 6.2681\n",
      "\b\n",
      " 3/17 [====>.........................]\n",
      " - ETA: 0s - loss: 6.5355 - mae: 6.5355\n",
      "\b\n",
      "11/17 [==================>...........]\n",
      " - ETA: 0s - loss: 6.2730 - mae: 6.2730\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 15ms/step - loss: 6.2798 - mae: 6.2798 - val_loss: 5.0304 - val_mae: 5.0304\n",
      "\n",
      "Epoch 289/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.6759 - mae: 6.6759\n",
      "\b\n",
      " 8/17 [=============>................]\n",
      " - ETA: 0s - loss: 6.4205 - mae: 6.4205\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 8ms/step - loss: 6.3008 - mae: 6.3008 - val_loss: 4.6019 - val_mae: 4.6019\n",
      "\n",
      "Epoch 290/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.9454 - mae: 6.9454\n",
      "\b\n",
      " 2/17 [==>...........................]\n",
      " - ETA: 0s - loss: 6.9901 - mae: 6.9901\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.4941 - mae: 6.4941\n",
      "\b\n",
      "15/17 [=========================>....]\n",
      " - ETA: 0s - loss: 6.3737 - mae: 6.3737\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 15ms/step - loss: 6.4055 - mae: 6.4055 - val_loss: 4.8856 - val_mae: 4.8856\n",
      "\n",
      "Epoch 291/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 7.0369 - mae: 7.0369\n",
      "\b\n",
      "11/17 [==================>...........]\n",
      " - ETA: 0s - loss: 6.1354 - mae: 6.1354\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 7ms/step - loss: 6.2631 - mae: 6.2631 - val_loss: 4.7701 - val_mae: 4.7701\n",
      "\n",
      "Epoch 292/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4972 - mae: 6.4972\n",
      "\b\n",
      " 6/17 [=========>....................]\n",
      " - ETA: 0s - loss: 6.2494 - mae: 6.2494\n",
      "\b\n",
      " 8/17 [=============>................]\n",
      " - ETA: 0s - loss: 6.3237 - mae: 6.3237\n",
      "\b\n",
      " 9/17 [==============>...............]\n",
      " - ETA: 0s - loss: 6.3331 - mae: 6.3331\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.3110 - mae: 6.3110\n",
      "\b\n",
      "15/17 [=========================>....]\n",
      " - ETA: 0s - loss: 6.2497 - mae: 6.2497\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 22ms/step - loss: 6.2304 - mae: 6.2304 - val_loss: 4.8532 - val_mae: 4.8532\n",
      "\n",
      "Epoch 293/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.5163 - mae: 6.5163\n",
      "\b\n",
      " 6/17 [=========>....................]\n",
      " - ETA: 0s - loss: 6.4241 - mae: 6.4241\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.2905 - mae: 6.2905\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 11ms/step - loss: 6.3324 - mae: 6.3324 - val_loss: 4.7475 - val_mae: 4.7475\n",
      "\n",
      "Epoch 294/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4505 - mae: 6.4505\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.1709 - mae: 6.1709\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 7ms/step - loss: 6.2743 - mae: 6.2743 - val_loss: 4.4344 - val_mae: 4.4344\n",
      "\n",
      "Epoch 295/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.9426 - mae: 6.9426\n",
      "\b\n",
      " 2/17 [==>...........................]\n",
      " - ETA: 0s - loss: 6.5907 - mae: 6.5907\n",
      "\b\n",
      " 3/17 [====>.........................]\n",
      " - ETA: 0s - loss: 6.6022 - mae: 6.6022\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.4280 - mae: 6.4280\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 19ms/step - loss: 6.3381 - mae: 6.3381 - val_loss: 4.3969 - val_mae: 4.3969\n",
      "\n",
      "Epoch 296/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.4660 - mae: 6.4660\n",
      "\b\n",
      " 7/17 [===========>..................]\n",
      " - ETA: 0s - loss: 6.3850 - mae: 6.3850\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.3083 - mae: 6.3083\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 11ms/step - loss: 6.3490 - mae: 6.3490 - val_loss: 4.8563 - val_mae: 4.8563\n",
      "\n",
      "Epoch 297/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.1594 - mae: 6.1594\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.2543 - mae: 6.2543\n",
      "\b\n",
      "12/17 [====================>.........]\n",
      " - ETA: 0s - loss: 6.2930 - mae: 6.2930\n",
      "\b\n",
      "16/17 [===========================>..]\n",
      " - ETA: 0s - loss: 6.2952 - mae: 6.2952\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 14ms/step - loss: 6.2925 - mae: 6.2925 - val_loss: 5.1909 - val_mae: 5.1909\n",
      "\n",
      "Epoch 298/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.6148 - mae: 6.6148\n",
      "\b\n",
      " 4/17 [======>.......................]\n",
      " - ETA: 0s - loss: 6.2122 - mae: 6.2122\n",
      "\b\n",
      "10/17 [================>.............]\n",
      " - ETA: 0s - loss: 6.1987 - mae: 6.1987\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - ETA: 0s - loss: 6.2203 - mae: 6.2203\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 15ms/step - loss: 6.2203 - mae: 6.2203 - val_loss: 4.8956 - val_mae: 4.8956\n",
      "\n",
      "Epoch 299/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 6.6345 - mae: 6.6345\n",
      "\b\n",
      " 2/17 [==>...........................]\n",
      " - ETA: 0s - loss: 6.3211 - mae: 6.3211\n",
      "\b\n",
      "11/17 [==================>...........]\n",
      " - ETA: 0s - loss: 6.3483 - mae: 6.3483\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 22ms/step - loss: 6.3785 - mae: 6.3785 - val_loss: 4.3617 - val_mae: 4.3617\n",
      "\n",
      "Epoch 300/300\n",
      " 1/17 [>.............................]\n",
      " - ETA: 0s - loss: 7.1887 - mae: 7.1887\n",
      "\b\n",
      " 8/17 [=============>................]\n",
      " - ETA: 0s - loss: 6.3266 - mae: 6.3266\n",
      "\b\n",
      " 9/17 [==============>...............]\n",
      " - ETA: 0s - loss: 6.2800 - mae: 6.2800\n",
      "\b\n",
      "17/17 [==============================]\n",
      " - 0s 14ms/step - loss: 6.2911 - mae: 6.2911 - val_loss: 4.7223 - val_mae: 4.7223\n",
      "\n",
      "1/2 [==============>...............]\n",
      " - ETA: 0s\n",
      "\n",
      "2/2 [==============================]\n",
      " - 0s 5ms/step\n",
      "\n",
      "MAE:\n",
      "4.7223262375178345\n",
      "100%|██████████| 1/1 [01:32<00:00, 92.92s/trial, best loss: -4.7223262375178345]\n",
      "best: \n",
      "{'activation': 0, 'batch_size': 0, 'dropout1': 0.4767828318268987, 'dropout2': 0.37254197666839417, 'dropout3': 0, 'layers_number': 1, 'learning_rate': 8.0, 'loss': 0, 'momentum': 0.09644045704118953, 'nb_epochs': 1, 'nesterov': 1, 'units1': 195.0, 'units2': 170.0, 'units3': 2, 'weight_init': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# (1) Load data\n",
    "\n",
    "feat_X = pd.read_pickle(f\"./variables/feat_X.pkl\")\n",
    "feat_y = pd.read_pickle(f\"./variables/feat_y.pkl\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "perc_test=0.1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(feat_X, feat_y, test_size=perc_test,shuffle=False, random_state=1236548)\n",
    "print('Number of samples in the training set:', X_train.shape[0])\n",
    "print('Number of samples in the test set:', X_val.shape[0])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "x_train_array = np.array(X_train, dtype = float)\n",
    "y_train_array = np.array(y_train)\n",
    "x_val_array = np.array(X_val, dtype = float)\n",
    "y_val_array = np.array(y_val)\n",
    "\n",
    "print(x_train_array.shape)\n",
    "print(x_val_array.shape)\n",
    "print(y_train_array.shape)\n",
    "print(y_val_array.shape)\n",
    "\n",
    "# (2) Trials object\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# (3) Run\n",
    "\n",
    "best = fmin(neural_net, space, algo=tpe.suggest, max_evals = 1, trials=trials)\n",
    "print('best: ')\n",
    "print(best)\n",
    "\n",
    "# (4) Save\n",
    "\n",
    "save_trials_path = Path('./')\n",
    "with open(save_trials_path / 'trials.pickle', 'wb') as pickle_file:\n",
    "    pickle.dump(trials, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'PRD' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cf9579b0180d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Forecast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PRD' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "x = PRD.index\n",
    "y = PRD\n",
    "ax.plot(x, y, label='Forecast')\n",
    "\n",
    "x = dam2.index\n",
    "y = dam2['Price']\n",
    "ax.plot(x, y, label='Actual')\n",
    "\n",
    "ax.set_title(f'Price forecast and realisation for test data');\n",
    "ax.set_xlabel(r'Date');\n",
    "ax.set_ylabel(r'Price');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.backend.clear_session()\n",
    "\n",
    "# def create_model():\n",
    "#     # initialize model\n",
    "#     model = keras.models.Sequential()\n",
    "\n",
    "#     # add input layer\n",
    "#     model.add(keras.layers.Dense(\n",
    "#         units=50,\n",
    "#         # input_dim=X_train_centered.shape[1],\n",
    "#         input_dim=50,\n",
    "#         kernel_initializer='glorot_uniform',\n",
    "#         bias_initializer='zeros',\n",
    "#         activation='tanh',\n",
    "#         kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "#     ))\n",
    "\n",
    "#     model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "#     # add hidden layer\n",
    "#     model.add(\n",
    "#         keras.layers.Dense(\n",
    "#             units=50,\n",
    "#             input_dim=50,\n",
    "#             kernel_initializer='glorot_uniform',\n",
    "#             bias_initializer='zeros',\n",
    "#             activation='tanh',\n",
    "#             kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "#         ))\n",
    "\n",
    "#     # add output layer\n",
    "#     model.add(\n",
    "#         keras.layers.Dense(\n",
    "#             # units=y_train_onehot.shape[1],\n",
    "#             units=5,\n",
    "#             input_dim=50,\n",
    "#             kernel_initializer='glorot_uniform',\n",
    "#             bias_initializer='zeros',\n",
    "#             activation='softmax',\n",
    "#             kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "#         ))\n",
    "\n",
    "#     # define SGD optimizer\n",
    "#     sgd_optimizer = keras.optimizers.SGD(\n",
    "#         lr=0.001, decay=1e-7, momentum=0.9\n",
    "#     )\n",
    "#     # compile model\n",
    "#     model.compile(\n",
    "#         optimizer=sgd_optimizer,\n",
    "#         loss='categorical_crossentropy'\n",
    "#     )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### HYPEROPT ###\n",
    "\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "# import sys\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# np.random.seed(6669)\n",
    "\n",
    "# from sklearn.model_selection import KFold, StratifiedKFold\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Dropout, Activation\n",
    "# from keras.optimizers import SGD, Adam\n",
    "# from keras.utils import np_utils\n",
    "# from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# import tensorflow as tf\n",
    "# tf.python.control_flow_ops = tf\n",
    "\n",
    "# # Based on Faron's stacker. Thanks!\n",
    "\n",
    "# ID = 'id'\n",
    "# TARGET = 'loss'\n",
    "# NFOLDS = 5\n",
    "# SEED = 669\n",
    "# NROWS = None\n",
    "# DATA_DIR = \"../../\"\n",
    "\n",
    "# TRAIN_FILE = \"./train.csv\"\n",
    "# TEST_FILE = \"./test.csv\"\n",
    "# SUBMISSION_FILE = \"./sample_submission.csv\"\n",
    "\n",
    "# train = pd.read_csv(TRAIN_FILE, nrows=NROWS)\n",
    "# test = pd.read_csv(TEST_FILE, nrows=NROWS)\n",
    "\n",
    "# train_indices = train[ID]\n",
    "# test_indices = test[ID]\n",
    "\n",
    "# y_train_full = train[\"loss\"]\n",
    "# y_train_ravel = train[TARGET].ravel()\n",
    "\n",
    "# train.drop([ID, TARGET], axis=1, inplace=True)\n",
    "# test.drop([ID], axis=1, inplace=True)\n",
    "\n",
    "# print(\"{},{}\".format(train.shape, test.shape))\n",
    "\n",
    "# ntrain = train.shape[0]\n",
    "# ntest = test.shape[0]\n",
    "# train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "# features = train.columns\n",
    "\n",
    "# cats = [feat for feat in features if 'cat' in feat]\n",
    "# for feat in cats:\n",
    "#     train_test[feat] = pd.factorize(train_test[feat], sort=True)[0]\n",
    "    \n",
    "# train = train_test.iloc[:ntrain, :]\n",
    "\n",
    "# # Using train_test_split in order to create random split for Keras,\n",
    "# # otherwise it'll use last part of data when\n",
    "# # validation_split is provided in the model parameters.\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train, y_train_full, test_size = 0.15)\n",
    "\n",
    "# feat_X = pd.read_pickle(f\"./variables/feat_X.pkl\")\n",
    "# feat_y = pd.read_pickle(f\"./variables/feat_y.pkl\")\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# perc_test=0.1\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(feat_X, feat_y, test_size=perc_test,shuffle=False, random_state=1236548)\n",
    "# print('Number of samples in the training set:', X_train.shape[0])\n",
    "# print('Number of samples in the test set:', X_val.shape[0])\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_val.shape)\n",
    "\n",
    "# x_train_array = np.array(X_train, dtype = float)\n",
    "# y_train_array = np.array(y_train)\n",
    "# x_val_array = np.array(X_val, dtype = float)\n",
    "# y_val_array = np.array(y_val)\n",
    "\n",
    "# print(x_train_array.shape)\n",
    "# print(x_val_array.shape)\n",
    "# print(y_train_array.shape)\n",
    "# print(y_val_array.shape)\n",
    "\n",
    "# # Unfortunately, I didn't manage to implement proper KFold when using Hyperopt.\n",
    "# # This can be done easily using GridSearch.\n",
    "# # Code for 5-fold CV in further section.\n",
    "\n",
    "\n",
    "# # Parameters search space, can be adjusted according to your needs.\n",
    "\n",
    "# space = { 'choice': hp.choice('layers_number',\n",
    "#                              [{'layers': 'two'},\n",
    "#                              {'layers': 'three',\n",
    "#                              'units3': hp.choice('units3', [32, 64, 256]),\n",
    "#                              'dropout3': hp.choice('dropout3', np.linspace(0.1, 0.3, 3, dtype=float))\n",
    "#                              }]),\n",
    "\n",
    "#             'units1': hp.choice('units1', [512, 768, 1024]),\n",
    "#             'units2': hp.choice('units2', [128, 256, 512]),\n",
    "#             #'units3': hp.choice('units3', [32, 64, 256]), \n",
    "\n",
    "#             'dropout1': hp.choice('dropout1', np.linspace(0.3, 0.5, 3, dtype=float)),\n",
    "#             'dropout2': hp.choice('dropout2', np.linspace(0.1, 0.3, 3, dtype=float)),\n",
    "#             #'dropout3': hp.choice('dropout3', np.linspace(0.1, 0.3, 3, dtype=float)),\n",
    "\n",
    "#             'batch_size' : hp.choice('batch_size', [128, 256, 512]),\n",
    "\n",
    "#             'nb_epochs' :  hp.choice('nb_epochs', [30, 50, 100]),\n",
    "            \n",
    "#         }\n",
    "\n",
    "\n",
    "# # Architecture of NN loosely based on Danijel Kivaranovic Keras script. Thanks!\n",
    "\n",
    "# def neural_net(params):   \n",
    "\n",
    "#     print ('Params testing: ', params)\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(params['units1'], input_dim = x_train_array.shape[1]))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     model.add(Dense(params['units2']))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(Dropout(params['dropout2']))\n",
    "\n",
    "#     if params['choice']['layers'] == 'three':\n",
    "#         model.add(Dense(params['choice']['units3'])) \n",
    "#         model.add(PReLU())\n",
    "#         model.add(Dropout(params['choice']['dropout3']))    \n",
    "\n",
    "#     model.add(Dense(24))\n",
    "#     model.add(Activation('linear'))\n",
    "#     model.compile(loss = 'mae', optimizer = 'adam', metrics = [\"mae\"])\n",
    "    \n",
    "\n",
    "#     model.fit(x_train_array, y_train_array, epochs=params['nb_epochs'],\n",
    "#               batch_size=params['batch_size'], verbose = 1, validation_data = (x_val_array, y_val_array))\n",
    "\n",
    "#     preds  = model.predict(x_val_array, batch_size = params['batch_size'], verbose = 1)\n",
    "#     acc = mean_absolute_error(y_val_array, preds)\n",
    "#     print('MAE:', acc)\n",
    "#     sys.stdout.flush() \n",
    "#     return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "# trials = Trials()\n",
    "# best = fmin(neural_net, space, algo=tpe.suggest, max_evals = 1, trials=trials)\n",
    "# print('best: ')\n",
    "# print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space = {'choice':\n",
    "\n",
    "\n",
    "# hp.choice('num_layers',\n",
    "#     [\n",
    "#                     {'layers':'two',\n",
    "                     \n",
    "                                                    \n",
    "#                     },\n",
    "        \n",
    "#                      {'layers':'three',\n",
    "                      \n",
    "                      \n",
    "#                       'units3': hp.choice('units3', [64, 128, 256, 512]),\n",
    "#                       'dropout3': hp.choice('dropout3', [0.25,0.5,0.75])\n",
    "                                \n",
    "#                     }\n",
    "        \n",
    "    \n",
    "#     ]),\n",
    "    \n",
    "#     'units1': hp.choice('units1', [64, 128, 256, 512]),\n",
    "#     'units2': hp.choice('units2', [64, 128, 256, 512]),\n",
    "                 \n",
    "#     'dropout1': hp.choice('dropout1', [0.25,0.5,0.75]),\n",
    "#     'dropout2': hp.choice('dropout2', [0.25,0.5,0.75]),\n",
    "    \n",
    "#     'batch_size' : hp.choice('batch_size', [28,64,128]),\n",
    "\n",
    "#     'nb_epochs' :  100,\n",
    "#     'optimizer': 'adadelta',\n",
    "#     'activation': 'relu'\n",
    "    \n",
    "    \n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neural_net(params):   \n",
    "\n",
    "#     print ('Params testing: ', params)\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(params['units1'], input_dim = x_train_array.shape[1]))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     model.add(Dense(params['units2']))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(Dropout(params['dropout2']))\n",
    "\n",
    "#     if params['choice']['layers'] == 'three':\n",
    "#         model.add(Dense(params['choice']['units3'])) \n",
    "#         model.add(PReLU())\n",
    "#         model.add(Dropout(params['choice']['dropout3']))    \n",
    "\n",
    "#     model.add(Dense(24))\n",
    "#     model.add(Activation('linear'))\n",
    "#     model.compile(loss = 'mae', optimizer = 'adam', metrics = [\"mae\"])\n",
    "    \n",
    "\n",
    "#     model.fit(x_train_array, y_train_array, epochs=params['nb_epochs'],\n",
    "#               batch_size=params['batch_size'], verbose = 1, validation_data = (x_val_array, y_val_array))\n",
    "\n",
    "#     preds  = model.predict(x_val_array, batch_size = params['batch_size'], verbose = 1)\n",
    "#     acc = mean_absolute_error(y_val_array, preds)\n",
    "#     print('MAE:', acc)\n",
    "#     sys.stdout.flush() \n",
    "#     return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Objective function that hyperopt will minimize\n",
    "\n",
    "# def root_mean_squared_error(y_true, y_pred):\n",
    "#     return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "# def objective(params):\n",
    "    \n",
    "#     # import ml_metrics\n",
    "    \n",
    "#     from keras.models import Sequential\n",
    "#     from keras.layers.core import Dense, Dropout, Activation\n",
    "#     from keras.optimizers import Adadelta\n",
    "#     from keras.layers.normalization import BatchNormalization\n",
    "#     from keras.callbacks import Callback\n",
    "\n",
    "#     from sklearn.metrics import mean_absolute_error as MAE\n",
    "#     from sklearn.metrics import mean_squared_error as MSE\n",
    "    \n",
    "#     print ('Params testing: ', params)\n",
    "#     print ('\\n ')\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(params['units1'], input_dim = X_train.shape[1]))\n",
    "#     model.add(Activation(params['activation']))\n",
    "#     model.add(Dropout(params['dropout1']))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(params['units2']))\n",
    "#     model.add(Activation(params['activation']))\n",
    "#     model.add(Dropout(params['dropout2']))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     if params['choice']['layers']== 'three':\n",
    "#         model.add(Dense(params['choice']['units3'])) \n",
    "#         model.add(Activation(params['activation']))\n",
    "#         model.add(Dropout(params['choice']['dropout3']))\n",
    "#         model.add(BatchNormalization())\n",
    "#         patience=25\n",
    "#     else:\n",
    "#         patience=15\n",
    "     \n",
    "#     model.add(Dense(1))    #End in a single output node for regression style output\n",
    "#     # model.compile(loss=root_mean_squared_error, optimizer=params['optimizer'])\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=params['optimizer'],\n",
    "#         loss='sparse_categorical_crossentropy'\n",
    "#     )\n",
    "    \n",
    "#     #object of class for call back early stopping \n",
    "#     # val_call = clsvalidation_kappa(validation_data=(X_val, y_val), patience=patience, filepath='\"../input/best.h5') #instantiate object\n",
    "\n",
    "#     #includes the call back object\n",
    "#     model.fit(X_train, y_train, epochs=params['nb_epochs'], batch_size=params['batch_size'], verbose = 0)\n",
    "     \n",
    "#     #predict the test set\n",
    "#     preds=model.predict(X_val, batch_size = 5000, verbose = 0)\n",
    "    \n",
    "#     predClipped = np.clip(np.round(preds.astype(int).ravel()), 1, 8) #simple rounding of predictionto int\n",
    "#     # score=ml_metrics.quadratic_weighted_kappa(y_test.values.ravel(),predClipped)\n",
    "\n",
    "#     score = MAE(y_val, preds)\n",
    " \n",
    "#     return {'loss': score, 'status': STATUS_OK, 'rounds': val_call.best_rounds}\n",
    "\n",
    "# trials = Trials()\n",
    "\n",
    "# best = fmin(objective, space, algo=tpe.suggest, trials=trials, max_evals=100)\n",
    "\n",
    "# print (best)\n",
    "# print (trials.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### HYPEROPT ###\n",
    "\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "# import sys\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# np.random.seed(6669)\n",
    "\n",
    "# from sklearn.model_selection import KFold, StratifiedKFold\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Dropout, Activation\n",
    "# from keras.optimizers import SGD, Adam\n",
    "# from keras.utils import np_utils\n",
    "# from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# import tensorflow as tf\n",
    "# tf.python.control_flow_ops = tf\n",
    "\n",
    "# # Based on Faron's stacker. Thanks!\n",
    "\n",
    "# ID = 'id'\n",
    "# TARGET = 'loss'\n",
    "# NFOLDS = 5\n",
    "# SEED = 669\n",
    "# NROWS = None\n",
    "# DATA_DIR = \"../../\"\n",
    "\n",
    "# TRAIN_FILE = \"./train.csv\"\n",
    "# TEST_FILE = \"./test.csv\"\n",
    "# SUBMISSION_FILE = \"./sample_submission.csv\"\n",
    "\n",
    "# train = pd.read_csv(TRAIN_FILE, nrows=NROWS)\n",
    "# test = pd.read_csv(TEST_FILE, nrows=NROWS)\n",
    "\n",
    "# train_indices = train[ID]\n",
    "# test_indices = test[ID]\n",
    "\n",
    "# y_train_full = train[\"loss\"]\n",
    "# y_train_ravel = train[TARGET].ravel()\n",
    "\n",
    "# train.drop([ID, TARGET], axis=1, inplace=True)\n",
    "# test.drop([ID], axis=1, inplace=True)\n",
    "\n",
    "# print(\"{},{}\".format(train.shape, test.shape))\n",
    "\n",
    "# ntrain = train.shape[0]\n",
    "# ntest = test.shape[0]\n",
    "# train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "\n",
    "# features = train.columns\n",
    "\n",
    "# cats = [feat for feat in features if 'cat' in feat]\n",
    "# for feat in cats:\n",
    "#     train_test[feat] = pd.factorize(train_test[feat], sort=True)[0]\n",
    "    \n",
    "# train = train_test.iloc[:ntrain, :]\n",
    "\n",
    "# # Using train_test_split in order to create random split for Keras,\n",
    "# # otherwise it'll use last part of data when\n",
    "# # validation_split is provided in the model parameters.\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train, y_train_full, test_size = 0.15)\n",
    "\n",
    "# feat_X = pd.read_pickle(f\"./variables/feat_X.pkl\")\n",
    "# feat_y = pd.read_pickle(f\"./variables/feat_y.pkl\")\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# perc_test=0.1\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(feat_X, feat_y, test_size=perc_test,shuffle=False, random_state=1236548)\n",
    "# print('Number of samples in the training set:', X_train.shape[0])\n",
    "# print('Number of samples in the test set:', X_val.shape[0])\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_val.shape)\n",
    "\n",
    "# x_train_array = np.array(X_train, dtype = float)\n",
    "# y_train_array = np.array(y_train)\n",
    "# x_val_array = np.array(X_val, dtype = float)\n",
    "# y_val_array = np.array(y_val)\n",
    "\n",
    "# print(x_train_array.shape)\n",
    "# print(x_val_array.shape)\n",
    "# print(y_train_array.shape)\n",
    "# print(y_val_array.shape)\n",
    "\n",
    "# # Unfortunately, I didn't manage to implement proper KFold when using Hyperopt.\n",
    "# # This can be done easily using GridSearch.\n",
    "# # Code for 5-fold CV in further section.\n",
    "\n",
    "\n",
    "# # Parameters search space, can be adjusted according to your needs.\n",
    "\n",
    "# space = { 'choice': hp.choice('layers_number',\n",
    "#                              [{'layers': 'two'},\n",
    "#                              {'layers': 'three',\n",
    "#                              'units3': hp.choice('units3', [32, 64, 256]),\n",
    "#                              'dropout3': hp.choice('dropout3', np.linspace(0.1, 0.3, 3, dtype=float))\n",
    "#                              }]),\n",
    "\n",
    "#             'units1': hp.choice('units1', [512, 768, 1024]),\n",
    "#             'units2': hp.choice('units2', [128, 256, 512]),\n",
    "#             #'units3': hp.choice('units3', [32, 64, 256]), \n",
    "\n",
    "#             'dropout1': hp.choice('dropout1', np.linspace(0.3, 0.5, 3, dtype=float)),\n",
    "#             'dropout2': hp.choice('dropout2', np.linspace(0.1, 0.3, 3, dtype=float)),\n",
    "#             #'dropout3': hp.choice('dropout3', np.linspace(0.1, 0.3, 3, dtype=float)),\n",
    "\n",
    "#             'batch_size' : hp.choice('batch_size', [128, 256, 512]),\n",
    "\n",
    "#             'nb_epochs' :  hp.choice('nb_epochs', [30, 50, 100]),\n",
    "            \n",
    "#         }\n",
    "\n",
    "\n",
    "# # Architecture of NN loosely based on Danijel Kivaranovic Keras script. Thanks!\n",
    "\n",
    "# def neural_net(params):   \n",
    "\n",
    "#     print ('Params testing: ', params)\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(params['units1'], input_dim = x_train_array.shape[1]))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     model.add(Dense(params['units2']))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(Dropout(params['dropout2']))\n",
    "\n",
    "#     if params['choice']['layers'] == 'three':\n",
    "#         model.add(Dense(params['choice']['units3'])) \n",
    "#         model.add(PReLU())\n",
    "#         model.add(Dropout(params['choice']['dropout3']))    \n",
    "\n",
    "#     model.add(Dense(24))\n",
    "#     model.add(Activation('linear'))\n",
    "#     model.compile(loss = 'mae', optimizer = 'adam', metrics = [\"mae\"])\n",
    "    \n",
    "\n",
    "#     model.fit(x_train_array, y_train_array, epochs=params['nb_epochs'],\n",
    "#               batch_size=params['batch_size'], verbose = 1, validation_data = (x_val_array, y_val_array))\n",
    "\n",
    "#     preds  = model.predict(x_val_array, batch_size = params['batch_size'], verbose = 1)\n",
    "#     acc = mean_absolute_error(y_val_array, preds)\n",
    "#     print('MAE:', acc)\n",
    "#     sys.stdout.flush() \n",
    "#     return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "# trials = Trials()\n",
    "# best = fmin(neural_net, space, algo=tpe.suggest, max_evals = 1, trials=trials)\n",
    "# print('best: ')\n",
    "# print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we install the necessary packages\n",
    "# !pip install networkx==1.11 # para instala hyperopt correctamente, si no, da errores\n",
    "# !pip install hyperopt\n",
    "# # necessary imports\n",
    "# import sys\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras.constraints import max_norm\n",
    "# from keras.optimizers import Adam\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.datasets import cifar10\n",
    "# SEED = 42\n",
    "# (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# validation_split = 0.1\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_split, random_state=SEED)\n",
    "# # Let's convert the data to float and then divide it by 255 to normalize it\n",
    "# # Due to image characteristics they can only get values from 0 to 255\n",
    "# X_train = X_train.astype('float32') / 255.\n",
    "# X_val = X_val.astype('float32') / 255.\n",
    "# X_test = X_test.astype('float32') / 255.\n",
    "# # let's convert the labels with one-hot encoding\n",
    "# n_classes = 10\n",
    "# y_train = to_categorical(y_train, n_classes)\n",
    "# y_val = to_categorical(y_val, n_classes)\n",
    "# y_test = to_categorical(y_test, n_classes)\n",
    "# # we define the search space\n",
    "# # we'll vary:\n",
    "# # - the number of filters in our conv layers\n",
    "# # - the dropout percentage\n",
    "# # - the number of neurons in the dense layer\n",
    "# space = {\n",
    "#     'n_filters_conv': hp.choice('n_filters_conv', [32, 64, 128]),\n",
    "#     'dropout': hp.uniform('dropout', 0.0, 0.5),\n",
    "#     'neurons_dense': hp.choice('neurons_dense', [256, 512, 1024]), \n",
    "# }\n",
    "# def get_callbacks(pars):\n",
    "#   callbacks = [EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=0, mode='auto')]\n",
    "#   return callbacks\n",
    "# def mi_cnn(pars):\n",
    "#   print ('Parameters: ', pars)\n",
    "#   model = Sequential()\n",
    " \n",
    "#   # First convolutional block\n",
    "#   model.add(Conv2D(pars['n_filters_conv'], kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "#   model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#   model.add(Dropout(pars['dropout']))\n",
    "# # second convolutional block\n",
    "#   model.add(Conv2D(pars['n_filters_conv'], kernel_size=(3, 3), activation='relu'))\n",
    "#   model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#   model.add(Dropout(pars['dropout']))\n",
    "# # third convolutional block\n",
    "#   model.add(Conv2D(pars['n_filters_conv'], kernel_size=(3, 3), activation='relu'))\n",
    "#   model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#   model.add(Dropout(pars['dropout']))\n",
    "# # Classifier block\n",
    "#   model.add(Flatten())\n",
    "#   model.add(Dense(pars['neurons_dense'], activation='relu', kernel_constraint=max_norm(3.)))\n",
    "#   model.add(Dropout(pars['dropout']))\n",
    "#   model.add(Dense(10, activation='softmax'))\n",
    "# # We compile the model\n",
    "#   model.compile(loss='categorical_crossentropy',\n",
    "#                 optimizer=Adam(lr=0.0001, decay=1e-6),\n",
    "#                 metrics=['accuracy'])\n",
    "# # We train the model\n",
    "#   history = model.fit(X_train, \n",
    "#                       y_train,\n",
    "#                       batch_size=128,\n",
    "#                       shuffle=True,\n",
    "#                       epochs=5,\n",
    "#                       validation_data=(X_val, y_val),\n",
    "#                       verbose = 0,\n",
    "#                       callbacks = get_callbacks(pars))\n",
    "# best_epoch_loss = np.argmin(history.history['val_loss'])\n",
    "# best_val_loss = np.min(history.history['val_loss'])\n",
    "# best_val_acc = np.max(history.history['val_acc'])\n",
    "\n",
    "# print('Epoch {} - val acc: {} - val loss: {}'.format(best_epoch_loss, best_val_acc, best_val_loss))\n",
    "# sys.stdout.flush()\n",
    "\n",
    "# return {'loss': best_val_loss, 'best_epoch': best_epoch_loss, 'eval_time': time.time(), 'status': STATUS_OK, 'model': model, 'history': history}\n",
    "# trials = Trials()\n",
    "# best = fmin(mi_cnn, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "# print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}