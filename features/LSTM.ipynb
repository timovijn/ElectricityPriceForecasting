{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Load necessary packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hex_salmon = '#F68F83'\n",
    "hex_gold = '#BC9661'\n",
    "hex_indigo = '#2D2E5F'\n",
    "hex_maroon = '#8C4750'\n",
    "hex_white = '#FAFAFA'\n",
    "hex_blue = '#7EB5D2'\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "import matplotlib.font_manager as font_manager\n",
    "mpl.font_manager._rebuild()\n",
    "\n",
    "mpl.rcParams['font.family'] = 'SF Mono'\n",
    "mpl.rcParams['font.weight'] = 'medium'\n",
    "mpl.rcParams['axes.titleweight'] = 'semibold'\n",
    "mpl.rcParams['axes.labelweight'] = 'medium'\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[hex_indigo, hex_salmon, hex_maroon])\n",
    "mpl.rcParams[\"figure.titlesize\"] = 'large'\n",
    "mpl.rcParams[\"figure.titleweight\"] = 'semibold'\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LogisticRegression, Ridge, ElasticNet, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from entsoe import EntsoePandasClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_pickle(f\"./features.pkl\")\n",
    "\n",
    "ID3 = pd.DataFrame()\n",
    "\n",
    "ID3['ID3'] = features['ID3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID3['ID3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ts_train_test(ID3, time_steps, for_periods):\n",
    "\n",
    "#     # Create training and test sets\n",
    "#     mask = (ID3.index.year < 2018)\n",
    "#     ts_train = ID3.loc[mask]['ID3'].values\n",
    "#     ts_train = ts_train.reshape(1, -1).transpose()\n",
    "\n",
    "#     mask = (ID3.index.year == 2018)\n",
    "#     ts_test = ID3.loc[mask]['ID3'].values\n",
    "#     ts_test = ts_test.reshape(1, -1).transpose()\n",
    "\n",
    "#     # ts_train = ID3[:'2018'].iloc['ID3'].values\n",
    "#     # ts_test  = ID3['2019':].iloc['ID3'].values\n",
    "\n",
    "#     # Scale the data\n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "#     sc = MinMaxScaler(feature_range=(0,1))\n",
    "#     ts_train_unscaled = ts_train\n",
    "#     ts_train = sc.fit_transform(ts_train)\n",
    "\n",
    "#     # Create training data of s samples and t time steps\n",
    "#     X_train = []\n",
    "#     y_train = []\n",
    "#     y_train_stacked = []\n",
    "    \n",
    "#     for i in range(time_steps, len(ts_train)-1): \n",
    "#         X_train.append(ts_train[i-time_steps:i,0])\n",
    "#         y_train.append(ts_train[i:i+for_periods,0])\n",
    "#     X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "#     # Reshape X_train for efficient modelling\n",
    "#     X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "#     # Prepare to create X_test\n",
    "#     inputs = pd.concat((ID3['ID3'][:'2018'], ID3['ID3']['2019':]),axis=0).values\n",
    "#     inputs = inputs[len(inputs)-len(ts_test) - time_steps:]\n",
    "#     inputs = inputs.reshape(-1,1)\n",
    "#     inputs  = sc.transform(inputs)\n",
    "\n",
    "#     X_test = []\n",
    "#     for i in range(time_steps, len(ts_test)+time_steps-for_periods):\n",
    "#         X_test.append(inputs[i-time_steps:i,0])\n",
    "        \n",
    "#     X_test = np.array(X_test)\n",
    "#     X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "#     return X_train, y_train , X_test, sc\n",
    "\n",
    "# ##########################################################################\n",
    "# X_train, y_train, X_test, sc = ts_train_test(ID3, 5, 2)\n",
    "# ##########################################################################\n",
    "\n",
    "# X_train.shape[0], X_train.shape[1]\n",
    "\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 5\n",
    "for_periods = 1\n",
    "\n",
    "lag = 4\n",
    "\n",
    "# Create training and test sets\n",
    "mask = (ID3.index.year < 2018)\n",
    "ts_train = ID3.loc[mask]['ID3'].values\n",
    "ts_train = ts_train.reshape(1, -1).transpose()\n",
    "\n",
    "mask = (ID3.index.year == 2018)\n",
    "ts_test = ID3.loc[mask]['ID3'].values\n",
    "ts_test = ts_test.reshape(1, -1).transpose()\n",
    "\n",
    "# Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "ts_train_unscaled = ts_train\n",
    "ts_train = sc.fit_transform(ts_train)\n",
    "\n",
    "# Create training data of s samples and t time steps\n",
    "X_train = []\n",
    "y_train = []\n",
    "y_train_stacked = []\n",
    "\n",
    "for i in range(time_steps+lag, len(ts_train)-1): \n",
    "    X_train.append(ts_train[i-lag-time_steps:i-lag,0])\n",
    "    y_train.append(ts_train[i:i+for_periods,0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "# Reshape X_train for efficient modelling\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Prepare to create X_test\n",
    "inputs = pd.concat((ID3['ID3'][:'2018'], ID3['ID3']['2019':]),axis=0).values\n",
    "inputs = inputs[len(inputs)-len(ts_test) - time_steps:]\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs  = sc.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(time_steps+lag, len(ts_test)+time_steps-for_periods):\n",
    "    X_test.append(inputs[i-lag-time_steps:i-lag,0])\n",
    "    y_test.append(ts_test[i:i+for_periods,0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "X_train.shape[0], X_train.shape[1]\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = features[['ID3']]\n",
    "# outputs = features[['ID3']]\n",
    "\n",
    "# inputs_train = inputs[inputs.index.year < 2018]\n",
    "# outputs_train = outputs[outputs.index.year < 2018]\n",
    "\n",
    "# inputs_test = inputs[inputs.index.year == 2018]\n",
    "# outputs_test = outputs[outputs.index.year == 2018]\n",
    "\n",
    "# # Scale inputs\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# inputs_train = pd.DataFrame(scaler.fit_transform(inputs_train), columns = inputs_train.columns)\n",
    "# inputs_test = pd.DataFrame(scaler.transform(inputs_test), columns = inputs_test.columns)\n",
    "\n",
    "# # Scale outputs\n",
    "# sc = MinMaxScaler()\n",
    "\n",
    "# sc.fit_transform(outputs_train)\n",
    "\n",
    "# outputs_train = pd.DataFrame(sc.fit_transform(outputs_train), columns = outputs_train.columns)\n",
    "# outputs_test = pd.DataFrame(sc.transform(outputs_test), columns = outputs_test.columns)\n",
    "\n",
    "# time_steps = 6\n",
    "# for_periods = 1\n",
    "# lag = 4\n",
    "\n",
    "# # X_train and y_train\n",
    "\n",
    "# X_train = []\n",
    "# y_train = []\n",
    "\n",
    "# for i in range(0, len(outputs_train)-time_steps-lag+1):\n",
    "#     for ii in range(0, time_steps):\n",
    "#         X_train.extend(inputs_train.iloc[i+ii].to_numpy())\n",
    "#     y_train.extend(outputs_train.iloc[i+time_steps+lag-1].to_numpy())\n",
    "\n",
    "# X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# X_train = X_train.reshape(y_train.shape[0], time_steps, inputs.shape[1])\n",
    "\n",
    "# # X_test and y_test\n",
    "\n",
    "# X_test = []\n",
    "# y_test = []\n",
    "\n",
    "# for i in range(0, len(outputs_test)-time_steps-lag+1):\n",
    "#     for ii in range(0, time_steps):\n",
    "#         X_test.extend(inputs_test.iloc[i+ii].to_numpy())\n",
    "#     y_test.extend(outputs_test.iloc[i+time_steps+lag-1].to_numpy())\n",
    "\n",
    "# X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# sc = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# X_test = X_test.reshape(y_test.shape[0], time_steps, inputs.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.fit_transform(outputs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0], X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 3-D shape of X_train to a data frame so we can see: \n",
    "X_train_see = pd.DataFrame(np.reshape(X_train, (X_train.shape[0], X_train.shape[1])))\n",
    "y_train_see = pd.DataFrame(y_train)\n",
    "pd.concat([X_train_see, y_train_see], axis=1)\n",
    "\n",
    "# Convert the 3-D shape of X_test to a data frame so we can see: \n",
    "X_test_see = pd.DataFrame(np.reshape(X_test, (X_test.shape[0], X_test.shape[1])))\n",
    "pd.DataFrame(X_test_see)\n",
    "\n",
    "print(), print(colored(f'There are {X_train.shape[0]} samples in the training data', 'green'))\n",
    "print(colored(f'There are {X_test.shape[0]} samples in the test data', 'green')), print()\n",
    "\n",
    "print(X_train_see.head(20))\n",
    "print(y_train_see.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "class Dropout(keras.layers.Dropout):\n",
    "  \"\"\"Applies Dropout to the input.\n",
    "  The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n",
    "  at each step during training time, which helps prevent overfitting.\n",
    "  Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over\n",
    "  all inputs is unchanged.\n",
    "  Note that the Dropout layer only applies when `training` is set to True\n",
    "  such that no values are dropped during inference. When using `model.fit`,\n",
    "  `training` will be appropriately set to True automatically, and in other\n",
    "  contexts, you can set the kwarg explicitly to True when calling the layer.\n",
    "  (This is in contrast to setting `trainable=False` for a Dropout layer.\n",
    "  `trainable` does not affect the layer's behavior, as Dropout does\n",
    "  not have any variables/weights that can be frozen during training.)\n",
    "  >>> tf.random.set_seed(0)\n",
    "  >>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n",
    "  >>> data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
    "  >>> print(data)\n",
    "  [[0. 1.]\n",
    "   [2. 3.]\n",
    "   [4. 5.]\n",
    "   [6. 7.]\n",
    "   [8. 9.]]\n",
    "  >>> outputs = layer(data, training=True)\n",
    "  >>> print(outputs)\n",
    "  tf.Tensor(\n",
    "  [[ 0.    1.25]\n",
    "   [ 2.5   3.75]\n",
    "   [ 5.    6.25]\n",
    "   [ 7.5   8.75]\n",
    "   [10.    0.  ]], shape=(5, 2), dtype=float32)\n",
    "  Args:\n",
    "    rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "    noise_shape: 1D integer tensor representing the shape of the\n",
    "      binary dropout mask that will be multiplied with the input.\n",
    "      For instance, if your inputs have shape\n",
    "      `(batch_size, timesteps, features)` and\n",
    "      you want the dropout mask to be the same for all timesteps,\n",
    "      you can use `noise_shape=(batch_size, 1, features)`.\n",
    "    seed: A Python integer to use as random seed.\n",
    "  Call arguments:\n",
    "    inputs: Input tensor (of any rank).\n",
    "    training: Python boolean indicating whether the layer should behave in\n",
    "      training mode (adding dropout) or in inference mode (doing nothing).\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, rate, training=None, noise_shape=None, seed=None, **kwargs):\n",
    "    super(Dropout, self).__init__(rate, noise_shape=None, seed=None, **kwargs)\n",
    "    self.training = training\n",
    "\n",
    "  def call(self, inputs, training=None):\n",
    "    if 0. < self.rate < 1.:\n",
    "        noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "        def dropped_inputs():\n",
    "            return K.dropout(inputs, self.rate, noise_shape, seed=self.seed)\n",
    "\n",
    "        if not training:\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training=self.training)\n",
    "        return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train, y_train, X_test, sc):\n",
    "    # create a model\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, SimpleRNN, GRU, LSTM\n",
    "    from keras.optimizers import SGD, Adam\n",
    "    \n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(units = 200, return_sequences = True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "    model_lstm.add(LSTM(units = 100, return_sequences = True))\n",
    "    model_lstm.add(LSTM(units = 50, return_sequences = False))\n",
    "    \n",
    "    model_lstm.add(Dense(units = 1, activation='linear'))\n",
    "\n",
    "    optimizer = Adam(clipvalue=0.5)\n",
    "    # model_lstm.compile(loss = 'mse', optimizer=optimizer)\n",
    "\n",
    "    # Compiling\n",
    "    model_lstm.compile(optimizer=optimizer,loss='mean_squared_error')\n",
    "    # Fitting to the training set\n",
    "    model_lstm.fit(X_train,y_train,epochs=1,batch_size=32, verbose=1)\n",
    "\n",
    "    LSTM_prediction = model_lstm.predict(X_test)\n",
    "    LSTM_prediction = sc.inverse_transform(LSTM_prediction)\n",
    "\n",
    "    # self.model = model_lstm\n",
    "\n",
    "    return model_lstm, LSTM_prediction\n",
    "\n",
    "def LSTM_model2(X_train, y_train, X_test, sc):\n",
    "    # create a model\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, SimpleRNN, GRU, LSTM\n",
    "    from keras.optimizers import SGD, Adam\n",
    "    \n",
    "    model_lstm = Sequential()\n",
    "    \n",
    "    model_lstm.add(LSTM(units = 200, return_sequences = True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "    \n",
    "    model_lstm.add(Dropout(0.2, training=True))\n",
    "    \n",
    "    model_lstm.add(LSTM(units = 100, return_sequences = False))\n",
    "\n",
    "    model_lstm.add(Dropout(0.2, training=True))\n",
    "    \n",
    "    # model_lstm.add(LSTM(units = 50, return_sequences = False))\n",
    "    \n",
    "    model_lstm.add(Dense(units = 1, activation='relu'))\n",
    "\n",
    "    optimizer = Adam(clipvalue=0.5)\n",
    "    # model_lstm.compile(loss = 'mse', optimizer=optimizer)\n",
    "\n",
    "    # Compiling\n",
    "    model_lstm.compile(optimizer=optimizer,loss='mean_squared_error')\n",
    "    # Fitting to the training set\n",
    "    model_lstm.fit(X_train,y_train,epochs=5,batch_size=32, verbose=1)\n",
    "\n",
    "    LSTM_prediction = model_lstm.predict(X_test)\n",
    "    LSTM_prediction = sc.inverse_transform(LSTM_prediction)\n",
    "\n",
    "    # self.model = model_lstm\n",
    "\n",
    "    return model_lstm, LSTM_prediction\n",
    "\n",
    "my_LSTM_model, LSTM_prediction = LSTM_model2(X_train, y_train, X_test, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM_prediction = sc.inverse_transform(LSTM_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_pred_plot(preds):\n",
    "\n",
    "    actual_pred = pd.DataFrame()\n",
    "\n",
    "    mask = (ID3.index.year == 2018)\n",
    "    actual_pred['Actual'] = ID3.loc[mask]['ID3'][0:len(preds)]\n",
    "    actual_pred['Predicted'] = preds[:, 0]\n",
    "\n",
    "    from keras.metrics import MeanSquaredError\n",
    "    m = MeanSquaredError()\n",
    "    actual_pred['Actual'] = actual_pred['Actual']\n",
    "    actual_pred['Predicted'] = actual_pred['Predicted']\n",
    "    m.update_state(np.array(actual_pred['Actual']), np.array(actual_pred['Predicted']))\n",
    "    \n",
    "    return (m.result().numpy(), actual_pred)\n",
    "\n",
    "m, prediction = actual_pred_plot(LSTM_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = features.drop(labels=['ID3', 'VOL', 'LOAD_F', 'LOAD_FE'], axis=1)\n",
    "# y = features['ID3']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X,\n",
    "#     y,\n",
    "#     test_size = 0.3,\n",
    "#     random_state = 0,\n",
    "#     shuffle = True)\n",
    "\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_unscaled = X_train\n",
    "# X_test_unscaled = X_test\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# X_train = scaler.transform(X_train_unscaled)\n",
    "\n",
    "# X_test = scaler.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MLP_model(X_train, y_train, X_test):\n",
    "#     # create a model\n",
    "#     from keras.models import Sequential\n",
    "#     from keras.layers import Dense\n",
    "#     from keras.optimizers import SGD, Adam\n",
    "    \n",
    "#     model_MLP = Sequential()\n",
    "#     model_MLP.add(Dense(200, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1],)))\n",
    "#     model_MLP.add(Dense(100, activation='relu', kernel_initializer='he_normal'))\n",
    "#     model_MLP.add(Dense(1))\n",
    "\n",
    "#     optimizer = Adam(clipvalue=0.5)\n",
    "#     # model_lstm.compile(loss = 'mse', optimizer=optimizer)\n",
    "\n",
    "#     # Compiling\n",
    "#     model_MLP.compile(optimizer=optimizer,loss='mean_squared_error')\n",
    "#     # Fitting to the training set\n",
    "#     model_MLP.fit(X_train, y_train, epochs = 50, batch_size = 16, verbose = 5)\n",
    "\n",
    "#     prediction = model_MLP.predict(X_test)\n",
    "#     # prediction = sc.inverse_transform(prediction)\n",
    "\n",
    "#     # self.model = model_lstm\n",
    "\n",
    "#     return model_MLP, prediction\n",
    "\n",
    "# model, prediction = MLP_model(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# #Step1. Define the model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(16, activation = 'relu', kernel_initializer = 'he_normal', input_shape = (X_train.shape[1],)))\n",
    "# model.add(Dense(8, activation = 'relu', kernel_initializer = 'he_normal'))\n",
    "# model.add(Dense(1))\n",
    "# #Step2. Compile the model\n",
    "# model.compile(optimizer = 'adam', loss = 'mse', metrics = 'mae')\n",
    "# #Step3. Fit the model\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100,batch_size=64, verbose=5)\n",
    "# #Step4.1 Evaluate the model\n",
    "# loss, mae = model.evaluate(X_test, y_test)\n",
    "# #Step4.2 Plot the learning curve\n",
    "# plt.plot(history.history['loss'], label='train')\n",
    "# plt.plot(history.history['val_loss'], label='val')\n",
    "# plt.show()\n",
    "\n",
    "# prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_pred = pd.DataFrame()\n",
    "\n",
    "# actual_pred['Actual'] = features['ID3'][0:len(prediction)]\n",
    "# actual_pred['Predicted'] = prediction[:, 0]\n",
    "\n",
    "# from keras.metrics import MeanSquaredError\n",
    "# m = MeanSquaredError()\n",
    "# actual_pred['Actual'] = actual_pred['Actual'].astype(int)\n",
    "# actual_pred['Predicted'] = actual_pred['Predicted'].astype(int)\n",
    "# m.update_state(np.array(actual_pred['Actual']), np.array(actual_pred['Predicted']))\n",
    "\n",
    "# x, y = actual_pred.index, actual_pred['Actual']\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (5 ,5))\n",
    "# ax.plot(x, y)\n",
    "\n",
    "# plt.xticks(rotation = 90)\n",
    "\n",
    "# x, y = actual_pred.index, actual_pred['Predicted']\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (5 ,5))\n",
    "# ax.plot(x, y)\n",
    "\n",
    "# plt.xticks(rotation = 90);\n",
    "\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Pre\n",
    "\n",
    "actual_pred = prediction\n",
    "\n",
    "# actual_pred = pd.DataFrame()\n",
    "\n",
    "# actual_pred['Actual'] = features['ID3'][0:len(prediction)]\n",
    "# actual_pred['Predicted'] = prediction[:, 0]\n",
    "\n",
    "####################################################################################################\n",
    "# Plot 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10 ,5))\n",
    "\n",
    "x, y = actual_pred[0:100].index, actual_pred[0:100]['Actual']\n",
    "\n",
    "ax.plot(x, y, linewidth=0.5)\n",
    "\n",
    "####################################################################################################\n",
    "# Plot 2\n",
    "\n",
    "x, y = actual_pred[0:100].index, actual_pred[0:100]['Predicted']\n",
    "\n",
    "ax.plot(x, y, alpha = 0.5)\n",
    "\n",
    "plt.xticks(rotation = 90);\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "actual = pd.DataFrame(['ID3'])\n",
    "predicted = pd.DataFrame(['ID3'])\n",
    "\n",
    "actual = pd.DataFrame(actual_pred['Actual'].values)\n",
    "actual['Type'] = 'Actual'\n",
    "predicted = pd.DataFrame(actual_pred['Predicted'].values)\n",
    "predicted['Type'] = 'Predicted'\n",
    "\n",
    "evaluation = pd.DataFrame(pd.concat([actual, \n",
    "                        predicted],\n",
    "                        axis=0))\n",
    "\n",
    "fig1 = go.Scatter(      x = actual_pred.index,\n",
    "                    y = actual_pred['Actual'],\n",
    "                    # color = evaluation['Type'],\n",
    "                    # title = \"Log of Appliance Energy Consumption in Wh vs Time\"\n",
    "                    )\n",
    "\n",
    "fig2 = go.Scatter(      x = actual_pred.index,\n",
    "                    y = actual_pred['Predicted'],\n",
    "                    # color = evaluation['Type'],\n",
    "                    # title = \"Log of Appliance Energy Consumption in Wh vs Time\"\n",
    "                    )\n",
    "\n",
    "data = [fig1, fig2]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = 50\n",
    "\n",
    "test_uncertainty_df = pd.DataFrame()\n",
    "# test_uncertainty_df['Date'] = testing_df['Date']\n",
    "\n",
    "for i in range(1, n_experiments + 1):\n",
    "  experiment_prediction = my_LSTM_model.predict(X_test)\n",
    "  test_uncertainty_df['ID3_{}'.format(i)] = np.concatenate(sc.inverse_transform(experiment_prediction), axis = 0)\n",
    "  \n",
    "  if i % 1 == 0:\n",
    "    print(f'Experiment: {i}/{n_experiments}')\n",
    "\n",
    "log_energy_consumption_df = test_uncertainty_df.filter(like='ID3', axis=1)\n",
    "test_uncertainty_df['ID3_mean'] = log_energy_consumption_df.mean(axis=1)\n",
    "test_uncertainty_df['ID3_std'] = log_energy_consumption_df.std(axis=1)\n",
    "\n",
    "test_uncertainty_df = test_uncertainty_df[['ID3_mean', 'ID3_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uncertainty_df['lower_bound'] = test_uncertainty_df['ID3_mean'] - 3*test_uncertainty_df['ID3_std']\n",
    "test_uncertainty_df['upper_bound'] = test_uncertainty_df['ID3_mean'] + 3*test_uncertainty_df['ID3_std']\n",
    "\n",
    "test_uncertainty_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# datespan = '2018-10-01', '2018-10-04'\n",
    "\n",
    "test_uncertainty_plot_df = test_uncertainty_df.copy(deep=True)\n",
    "# test_uncertainty_plot_df = test_uncertainty_plot_df.loc[test_uncertainty_plot_df['Date'].between('2016-08-01', '2018-10-04')]\n",
    "# truth_uncertainty_plot_df = testing_truth_df.copy(deep=True)\n",
    "# truth_uncertainty_plot_df = truth_uncertainty_plot_df.loc[testing_truth_df['Date'].between('2016-08-01', '2018-10-04')]\n",
    "\n",
    "upper_trace = go.Scatter(\n",
    "    x=actual_pred.index,\n",
    "    y=test_uncertainty_plot_df['upper_bound'],\n",
    "    mode='lines',\n",
    "    fill=None,\n",
    "    name='99% Upper Confidence Bound'\n",
    "    )\n",
    "\n",
    "lower_trace = go.Scatter(\n",
    "    x=actual_pred.index,\n",
    "    y=test_uncertainty_plot_df['lower_bound'],\n",
    "    mode='lines',\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(255, 211, 0, 0.5)',\n",
    "    name='99% Lower Confidence Bound'\n",
    "    )\n",
    "\n",
    "# real_trace = go.Scatter(\n",
    "#     x=truth_uncertainty_plot_df.index,\n",
    "#     y=truth_uncertainty_plot_df['ID3'],\n",
    "#     mode='lines',\n",
    "#     fill=None,\n",
    "#     name='Real Values'\n",
    "#     )\n",
    "\n",
    "# y_test = [item for sublist in y_test for item in sublist]\n",
    "\n",
    "real_trace = go.Scatter(\n",
    "    x=actual_pred.index,\n",
    "    y=actual_pred['Actual'],\n",
    "    mode='lines',\n",
    "    fill=None,\n",
    "    name='Real Values'\n",
    "    )\n",
    "\n",
    "mean_trace = go.Scatter(\n",
    "    x=actual_pred.index,\n",
    "    y=test_uncertainty_plot_df['ID3_mean'],\n",
    "    mode='lines',\n",
    "    fill=None,\n",
    "    name='Mean Values'\n",
    "    )\n",
    "\n",
    "# what_trace = go.Scatter(\n",
    "#     x=test_uncertainty_plot_df.index,\n",
    "#     y=np.concatenate(inverse_transform(bayesian_lstm.predict(X_test)), axis=0),\n",
    "#     mode='lines',\n",
    "#     fill=None,\n",
    "#     name='What Values'\n",
    "#     )\n",
    "\n",
    "data = [upper_trace, lower_trace, mean_trace, real_trace]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(title='Uncertainty Quantification for Energy Consumption Test Data',\n",
    "                   xaxis_title='Time',\n",
    "                   yaxis_title='log_energy_consumption (log Wh)')\n",
    "\n",
    "fig.layout.font.family = 'SF Mono'\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame()\n",
    "\n",
    "# Using 99% confidence bounds\n",
    "bounds_df['lower_bound'] = test_uncertainty_plot_df['lower_bound']\n",
    "bounds_df['prediction'] = test_uncertainty_plot_df['log_energy_consumption_mean']\n",
    "bounds_df['real_value'] = truth_uncertainty_plot_df['log_energy_consumption']\n",
    "bounds_df['upper_bound'] = test_uncertainty_plot_df['upper_bound']\n",
    "\n",
    "bounds_df['contained'] = ((bounds_df['real_value'] >= bounds_df['lower_bound']) &\n",
    "                          (bounds_df['real_value'] <= bounds_df['upper_bound']))\n",
    "\n",
    "print(\"Proportion of points contained within 99% confidence interval:\", \n",
    "      bounds_df['contained'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Plot 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10 ,5))\n",
    "\n",
    "x, y = actual_pred.index, actual_pred['Actual']\n",
    "\n",
    "ax.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Plot 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10 ,5))\n",
    "\n",
    "x, y = actual_pred.index, actual_pred['Predicted']\n",
    "\n",
    "ax.plot(x, y, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "\n",
    "smape(actual_pred['Actual'], actual_pred['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}